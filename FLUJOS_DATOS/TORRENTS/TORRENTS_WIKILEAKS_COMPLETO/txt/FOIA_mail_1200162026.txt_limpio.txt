from john lanzante johnlanzantenoaagov to santer1llnlgov john lanzante johnlanzantenoaagov subject re updated figures date sat 12 jan 2008 132026 0500 replyto johnlanzantenoaagov cc melissa free melissafreenoaagov peter thorne peterthornemetofficegovuk dian seidel dianseidelnoaagov tom wigley wigleycgducaredu karl taylor taylor13llnlgov thomas r karl thomasrkarlnoaagov carl mears mearsremsscom david c bader bader2llnlgov francis w zwiers franciszwiersecgcca frank wentz frankwentzremsscom leopold haimberger leopoldhaimbergerunivieacat michael c maccracken mmaccraccomcastnet phil jones pjonesueaacuk steve sherwood stevensherwoodyaleedu steve klein klein21mailllnlgov susan solomon susansolomonnoaagov tim osborn tosbornueaacuk gavin schmidt gschmidtgissnasagov hack james j jhackornlgov dear ben and all after returning to the office earlier in the week after couple of weeks off during the holidays i had the best of intentions of responding to some of the earlier emails unfortunately it has taken the better part of the week for to shovel out my avalanche of email this has lot to do with the remarkable progress that has been made kudos to ben and others who have made this possible at this point id like to add my 2 cents worth although with the declining dollar im not sure its worth that much any more on several issues some from earlier email and some from the last day or two i had given some thought as to where this article might be submitted although that issue has been settled ijc id like to add few related thoughts regarding the focus of the paper i think ben has brokered the best possible deal an expedited paper in ijc that is not treated as comment but im little confused as to whether our paper will be titled comments on by douglass et or whether we have bit more latitude while im not suggesting anything beyond short paper it might be possible to spin this in more general terms as brief update while at the same time addressing douglass et as part of this we could begin in the introduction by saying that this general topic has been much studied and debated in the recent past eg nrc 2000 the science 2005 papers and ccsp 2006 but that new developments since these works warrant revisiting the issue we could consider douglass et as one of several new developments we could perhaps title the paper something like revisiting temperature trends in the atmosphere the main conclusion will be that in stark contrast to douglass et the new evidence from the last couple of years has strengthened the conclusion of ccsp 2006 that there is meaningful discrepancy between models and observations in an earlier email ben suggested an outline for the paper 1 point out flaws in the statistical approach used by douglass et 2 show results from significance testing done properly 3 show figure with different estimates of radiosonde temperature trends illustrating the structural uncertainty 4 discuss complementary evidence supporting the finding that the tropical lower troposphere has warmed over the satellite era i think this is fine but id like to suggest couple of other items first some mention could be made regarding the structural uncertainty in satellite datasets we could have 3a for sondes and 3b for satellite data the satellite issue could be handled in as briefly as paragraph or with bit more work and discussion figure or table with some trends the main point to get across is that its not just uah vs rss with an implied edge to uah because its trends agree better with sondes its actually uah vs all others rss umd and zou et there are complications in adding umd and zou et to the discussion but these can be handled either qualitatively or quantitatively the complication with umd is that it only exists for t2 which has stratospheric influences and umd does not have corresponding measure for t4 which could be used to remove the stratospheric effects the complication with zou et is that the data begin in 1987 rather than 1979 as for the other satellite products it would be possible to use the fu method to remove the stratospheric influences from umd using t4 measures from either or both uah and rss it would be possible to directly compare trends from zou et with uah rss umd for time period starting in 1987 so in theory we could include some trend estimates from all 4 satellite datasets in apples vs apples comparisons but perhaps this is more work than is warranted for this project then at very least we can mention that in apples vs apples comparisons made in ccsp 2006 umd showed more tropospheric warming than both uah and rss and in comparisons made by zou et their dataset showed more warming than both uah and rss taken together this evidence leaves uah as the outlier compared to the other 3 datasets furthermore better trend agreement between uah and some sonde data is not necessarily good since the sonde data in question are likely to be afflicted with considerable spurious cooling biases the second item that id suggest be added to bens earlier outline perhaps as item 5 is discussion of the issues that susan raised in earlier emails the main point is that there is now some evidence that inadequacies in the ar4 model formulations pertaining to the treatment of stratospheric ozone may contribute to spurious cooling trends in the troposphere regarding bens fig 1 this is very nice graphical presentation of the differences in methodology between the current work and douglass et however i would suggest cautionary statement to the effect that while error bars are useful for illustrative purposes the use of overlapping error bars is not advocated for testing statistical significance between two variables following lanzante 2005 lanzante j r 2005 cautionary note on the use of error bars journal of climate 1817 36993703 this is also motivation for application of the twosample test that ben has implemented ben wrote so why is there small positive bias in the empiricallydetermined rejection rates karl believes that the answer may be partly linked to the skewness of the empiricallydetermined rejection rate distributions nb this is in regard to bens fig 3 which shows that the rejection rate in simulations using synthetic data appears to be slightly positively biased compared to the nominal expected rate i would note that the distribution of rejection rates is like the distribution of precipitation in that it is bounded by zero quickanddirty way to explore this possibility using trick used with precipitation data is to apply square root transformation to the rejection rates average these then reverse transform the average the square root transformation should yield data that is more nearly gaussian than the untransformed data ben wrote figure 3 as mike suggested ive removed the legend from the interior of the figure its now below the figure and have added arrows to indicate the theoreticallyexpected rejection rates for 5 10 and 20 tests as dian suggested ive changed the colors and thicknesses of the lines indicating results for the paired trends visually attention is now drawn to the results we think are most reasonable the results for the paired trend tests with standard errors adjusted for temporal autocorrelation effects i actually liked the earlier version of fig 3 better in some regards the labeling is now rather busy how about going back to dotted thin and thick curves to designate 5 10 and 20 and also placing labels 51020 on or near each curve then using just three colors to differentiate between douglass pairedno_se_adj and pairedwith_se_adj it will only be necessary to have 3 legends one for each of the three colors this would eliminate most of the legends another topic of recent discussion is what radiosonde datasets to include in the trend figure my own personal preference would be to have all available datasets shown in the figure however i would defer to the individual dataset creators if they feel uncomfortable about including sets that are not yet published peter also raised the point about trends being derived differently for different datasets to the extent possible it would be desirable to have things done the same for all datasets this is especially true for using the same time period and the same method to perform the regression another issue is the conversion of station data to areaaveraged data its usually easier to insure consistency if one person computes the trends from the raw data using the same procedures rather than having several people provide the trend estimates karl taylor wrote the lower panel of figure 2 by chance the mean of the results is displaced negatively i contend that the likelihood of getting difference of x is equal to the likelihood of getting difference of x i would like to see each difference plotted twice once with positive sign and again with negative sign one of the unfortunate problems with the asymmetry of the current figure is that to casual reader it might suggest consistency between the intraensemble distributions and the modelobs distributions that is not real ben and i have already discussed this point and i think were both still bit unsure on whats the best thing to do here perhaps others can provide convincing arguments for keeping the figure as is or making it symmetric as i suggest i agree with karl in regard to both his concern for misinterpretation as well as his suggested solution in the limit as n goes to infinity we expect the distribution to be symmetric since were comparing the model data with itself the problem we are encountering is due to finite sample effects for simplicity ben used limited number of unique combinations using full bootstrapping the problem should go away karls suggestion seems like simple and effective way around the problem karl taylor wrote it would appear that if we believe fgoals or miroc then the differences between many of the model runs and obs are not likely to be due to chance alone but indicate real discrepancy this would seem to indicate that our conclusion depends on which model ensembles we have most confidence in given the tiny sample sizes im not sure one can make any meaningful statements regarding differences between models particularly with regard to some measure of variability such as is implied by the width of distribution this raises another issue regarding fig 2 why show the results separately for each model this does not seem to be relevant to this project our objective is to show that the models as collection are not inconsistent with the observations not that any particular model is more or less consistent with the observations furthermore showing results for different models tempts the reader to make such comparisons why not just aggregate the results over all models and produce histogram this would also simplify the figure best regards _____john