estimating the uncertainty of the mann et 1998 1999 reconstructions introduction here i only consider the reconstruction of full northern hemisphere nh annualmean temperature as published by mann et 1998 1999 back to 1000 mann et have already published uncertainty or error ranges which are time dependent ie they widen or narrow as the set of proxy records with data values changes through time these are based on the calibration residuals fitting two white noise levels to their spectra to the frequency bands 0 to 002 and 002 to 05 and then integrating each to obtain the variance of the calibration residuals in the low and high frequency bands these variances are then summed and squarerooted to obtain the one standard error uncertainty range i have two requirements that are not met by these published uncertainties first i would like to know how they vary with time scale after all if the uncertainty is at least partly random error then the error of say 10year mean will be less than the error of individual yearly reconstructed values second i would like to be able to model the residuals including their spectral structure for situations where the standard error is insufficient eg estimating trends mike kindly sent the calibration residuals for the 1000 1400 and 1600 networks and i already had them for the full 1820 network i have used these to investigate the timedependent and timescaledependent nature of that part of the reconstruction errors that are expressed by the calibration residuals annual errors from the calibration residuals first i simply compared the standard deviation of the residuals 79 values from 19021980 with the standard errors provided by mike previously estimated as described above network mann et k residuals k 1000 0240 0232 1400 0244 0225 1450 0246 1500 0244 1600 0133 0194 1650 0122 1700 0124 1750 0129 1760 0122 1780 0116 1820 0113 0097 the values match reasonable well for the 1000 1400 and 1820 networks though im not sure why they do not match perfectly but there is an important disagreement for the 1600 network with the published errors much smaller than the calibration residuals imply mike do you have any explanation for this difference variance inflation factors if the calibration residuals are random errors then this aspect of the reconstruction uncertainty will reduce with time scale the variance of timemean of length m will be pic 1 so that the standard deviation reduces with m but increases with v the variance inflation factor v takes into account the autocorrelation structure of the calibration residuals if all residuals are independent of other residuals then v 1 if this is not the case then there are various formulae commonly used according the how large m is and according to how the autocorrelation structure is modelled eg with an autoregressive model these can all be derived from the most general formula that i am aware of pic 2 where rk is the autocorrelation at lag k an alternative formula uses the spectral density of the calibration residuals at 0 but is less general because it is only for large m pic 3 von storch and zwiers discuss three methods of estimating v using the estimated autocorrelation function acf directly in 2 performs very poorly probably because the acf is poorly known when the data sample is small as it is here fitting an autoregressive model of order p to the acf and then using 2 with the theoretical acf of the arp model or equivalently using the arp version of equation 2 performs better estimating the spectral density near to 0 and then using 3 was considered to be best for moderate to small samples i chose the second of these methods however because the spectral method does not provide an easy way of modelling the residuals and it also relies on the spectral density estimate near to 0 being the same as the spectral density at 0 ie that it is in the white noise part of the spectrum which may not be true given the small sample used here autocorrelation structure of the calibration residuals figure 1 shows the calibration residuals for the four networks considered plus the acf of each networks 1600 and 1820 are easiest to deal with since they have statistically significant autocorrelations even though they are not significant the sample lag1 autocorrelations are still better estimate of the population r1 than is zero in fact i use the data from subsequent lags to obtain the bestfit ar1 model to the acf virtually the same as the sample r1 for the 1000 and 1400 networks however there are some at least significant correlations in the acf the akaike information criterion suggests that an ar2 model is most appropriate for both cases the plots show ar1 and ar2 fits to the acf either estimated from just r1 or r1 and r2 respectively or as best fits to the whole acf neither adequately captures the positive correlations at the longer lags also the theoretical spectra of the ar2 models have unwanted structure with broad minimum in power between about 10 and 25 years the green lines in figure 1 show linear fits to the acfs which capture the characteristics of the acf quite well for the 1000 and 1400 networks these linear fits from lag1 to lag16 are padded with zero correlations out to lag40 and then an ar40 model is fit to them the autoregressive coefficients look wellbehaved ie changing smoothly with order and the theoretical spectra of the ar40 model is also wellbehaved being essentially white noise at time scales shorter than 20 years and then very red at longer time scales the peak at 0 matches that given be mann et 1999 in their figure 2 for the 1000 network for the 1000 and 1400 networks the ar40 model fit to this linear fit to the acf is used to represent the autocorrelation structure of the calibration residuals note that this is very different to fitting an ar40 model directly to the acf out to lag40 which would have 40 degrees of freedom and end up with falsely good fit to the data here there are only two degrees of freedom the intercept and slope of the linear fit to the acf standard errors at different time scales the autoregressive models ar40 fit to linear acf for 1000 and 1400 ar1 fit to acf for 1600 and 1820 are used with equation 2 to estimate the variance inflation factor for various time scales and then applied in equation 1 to estimate standard errors at these time scales variance inflation factor of each network and time scale timescale m 1000 1400 1600 1820 years 1 100 100 100 100 10 247 310 144 111 30 358 480 144 111 50 382 517 144 111 100 400 545 144 111 standard error of each network and time scale k timescale m 1000 1400 1600 1820 years 1 0232 0225 0194 0097 10 0115 0125 0074 0032 30 0080 0090 0043 0019 50 0064 0072 0033 0015 100 0046 0052 0023 0010 the standard errors are plotted in figure 2 for the different time scales and it is assumed that the 1000 errors are appropriate for 10001399 the 1400 errors for 14001599 the 1600 errors for 16001819 and the 1820 errors for 18201980 the third are these assumptions is likely to be wrong since there were big increases in the proxy network at intermediate stages between 1600 and 1819 nevertheless it allows to provide an uncertainty estimate for the whole reconstruction period and for comparison with the mann et errors shown in black in figure 2 the annual errors are similar to mann et except for the 1600 network as already noted above the errors reduce with time scale though note the slower reduction for the 1400 network due to the higher acf these errors can now be plotted both sides of the reconstruction with the reconstruction filtered with various length filters and using the errors appropriate to that time scale figure 3 shows both 1 and 2 standard errors the reduction with increasing time scale is very clear and the errors become tiny for 100year smoothing discussion while it is important to reduce the errors in an appropriate way with increasing time scale ie filtering or smoothing im not convinced that the errors estimated here are correct they seem to reduce too much with time scale on the basis that they become lower than the instrumental record standard errors jones et 1997 estimated the nh decadal mean to have standard error of between 0093 and 0054 k depending on instrumental data coverage perhaps the reason is that the calibration residuals capture only one part of the reconstruction error the other part comes from the uncertainty in the multivariate model parameters which is i guess hard to quantify in this case these other errors do not typically reduce with increasing time scale and because they are combined in quadrature ie as variances they can reduce the time scale sensitivity of the total uncertainty range some questions 1 from what i can tell from the various mann et papers that show these results you use the same uncertainty range ie the annual error even when showingcomparing smoothed reconstruction results is this true and what is the reason for this 2 perhaps it is because it is not possible to quantify the parameter uncertainty and hence by not reducing their errors with time scale they are being conservative about this that may be reasonable though they would be compensating for one bias by introducing another 3 perhaps there is mistake in my method or better alternative method 4 perhaps the errors computed here are indeed correct reflection of the reconstruction uncertainty any comments or feedback would be gratefully received pic figure 1 calibration residuals from 4 proxy networks plus acf and various fits to the acf pic figure 2 estimated reconstruction errors black from mann et 1999 purple from standard deviation of calibration residuals others from standard deviation of calibration residuals reduced according to equations 1 and 2 for averaging periods 10 pink 30 red 50 orange and 100 green years pic pic figure 3 reconstructions with various filters applied plus the estimate errors appropriate for each filter 1 year ie filter 10 year 30 year 50 year and 100 year smoothing