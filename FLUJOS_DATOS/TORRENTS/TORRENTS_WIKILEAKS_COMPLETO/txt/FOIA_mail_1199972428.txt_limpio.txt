from dianseidelnoaagov to santer1llnlgov subject re update on response to douglass et date thu 10 jan 2008 084028 0500 cc tom wigley wigleycgducaredu karl taylor taylor13llnlgov thomas r karl thomasrkarlnoaagov john lanzante johnlanzantenoaagov carl mears mearsremsscom david c bader bader2llnlgov francis w zwiers franciszwiersecgcca frank wentz frankwentzremsscom leopold haimberger leopoldhaimbergerunivieacat melissa free melissafreenoaagov michael c maccracken mmaccraccomcastnet philip d jones pjonesueaacuk steven sherwood stevensherwoodyaleedu steve klein klein21mailllnlgov susan solomon ssolomonalnoaagov thorne peter peterthornemetofficegovuk tim osborn tosbornueaacuk gavin schmidt gschmidtgissnasagov hack james j jhackornlgov dear ben thank you for this detailed update of your work few thoughts for your consideration where to submit this although i understand your and phils reluctance to try ijc it seems to that despite the new work presented this is really comment on douglass et and so rightly belongs in ijc if you suspect the review and publication process there is unacceptably long perhaps this should be confirmed by inquiring with the editor as professional courtesy decide in advance what youd consider reasonable turnaround time and if the editor says it will take longer going with another journal makes sense figures they look great as usual youve done super job telling the story in pictures one suggestion would be to indicate in fig 3 which test or trio of tests is the most appropriate now it is shown as the blue curves but id suggest making these black and the black ones blue and thicker than the rest that way those readers who just skim the paper and look at the figures will get the message quickly observations have you considered including results from hadat and ratpac as well as raobcor for even greater completeness version of ratpac pared down based on the results of randel and wu could be added as could steve sherwoods adjusted radiosonde data id suggest adding results from these datasets to your fig 1 not the planned fig 4 which i gather is meant to show the differences in versions of raobcor and the impact of douglass et als choice to use and early version with best wishes dian original message from ben santer santer1llnlgov date wednesday january 9 2008 1052 pm subject update on response to douglass et dear folks i just wanted to update you on my progress in formulating response to the douglass et paper in the international journal of climatology ijc there have been several developments first i contacted science to gauge their level of interest in publishing response to douglass et i thought it was worthwhile to test the water before devoting lot of time to the preparation of manuscript for submission to science i spoke with jesse smith who handles most of the climaterelated papers at science magazine the bottom line is that while science is interested in this issue particularly since douglass et are casting doubt on the findings of the 2005 santer et science paper jesse smith thought it was highly unlikely that science would carry rebuttal of work published in different journal ijc regretfully i agree our response to douglass et does not contain any fundamentally new science although it does contain some new and interesting work see below its an unfortunate situation singer is promoting the douglass et paper as startling new scientific evidence which undercuts the key conclusions of the ipcc and ccsp reports christy is using the douglass et paper to argue that his uah group is uniquely positioned to perform hardnosed and objective evaluation of model performance and that its dangerous to leave model evaluation in the hands of biased modelers much as i would like to see highprofile rebuttal of douglass et in journal like science or nature its unlikely that either journal will publish such rebuttal so what are our options personally id vote for grl i think that it is important to publish an expeditious response to the statistical flaws in douglass et in theory grl should be able to give us the desired fast turnaround time would grl accept our contribution given that the douglass et paper was published in ijc i think they would weve done substantial amount of new work see below and can argue with some justification that our contribution is more than just rebuttal of douglass et why not go for publication of response in ijc according to phil this option would probably take too long id be interested to hear any other thoughts you might have on publication options now to the science with lowercase s im appending three candidate figures for grl paper the first figure was motivated by discussions ive had with karl taylor and tom wigley its an attempt to convey the differences between our method of comparing observed and simulated trends panel and the approach used by douglass et panel b in our method we account for both statistical uncertainties in fitting leastsquares linear trends to noisy temporallyautocorrelated data and for the effects of internallygenerated variability as ive described in previous emails we compare each of the 49 simulated t2 and t2lt trends ie the same multimodel ensemble used in our 2005 science paper and in the 2006 ccsp report with observed t2 and t2lt trends obtained from the rss and uah groups our 2sigma confidence intervals on the model and observed trends are estimated as in santer et 2000 santer bd tml wigley js boyle dj gaffen jj hnilo d nychka parker and ke taylor 2000 statistical significance of trends and trend differences in layeraverage atmospheric temperature time series j geophys res 105 7337 7356 the method that santer et 2000 used to compute adjusted trend confidence intervals accounts for the fact that after fitting trend to t2 or t2lt data the regression residuals are typically highly autocorrelated if this autocorrelation is not accounted for one could easily reach incorrect decisions on whether the trend in an individual time series is significantly different from zero or whether two time series have significantly different trends santer et 2000 accounted for temporal autocorrelation effects by estimating r1 the lag1 autocorrelation of the regression residuals using r1 to calculate an effective sample size ne and then using ne to determine an adjusted standard error of the leastsquares linear trend panel of figure 1 shows the 2sigma adjusted standard errors for each individual trend models with excessively large tropical variability like fgoalsg10 and gfdlcm21 have large adjusted standard errors models with coarseresolution ogcms and low amplitude enso variability like the gissaom have smaller than observed adjusted standard errors neglect of volcanic forcing ie absence of chichon and pinatuboinduced temperature variability can also contribute to smaller than observed standard errors as in cccmacgcm31t47 the dark and light grey bars in panel show respectively the 1 and 2sigma standard errors for the rss t2lt trend as is visually obvious 36 of the 49 model trends are within 1 standard error of the rss trend and 47 of the 49 model trends are within 2 standard errors of the rss trend ive already explained our paired trend test procedure for calculating the statistical significance of the modelversusobserved trend differences this involves the normalized trend difference d1 d1 bo bm sqrt sbo2 sbm2 where bo and bm represent any single pair of observed and modeled trends with adjusted standard errors sbo and sbm under the assumption that d1 is normally distributed values of d1 196 or 196 indicate observedminusmodel trend differences that are significant at some stipulated significance level and one can easily calculate pvalue for each value of d1 these pvalues for the 98 pairs of trend tests 49 involving uah data and 49 involving rss data are what we use for determining the total number of hits or rejections of the null hypothesis of significant difference between modeled and observed trends i note that each test is twotailed since we have information priori about the direction of the model trend ie whether we expect the simulated trend to be significantly larger or smaller than observed rejection rates for paired trend tests obsvsmodel stipulated sign level of tests t2 hits t2lt hits 5 49 x 2 98 2 204 1 102 10 49 x 2 98 4 408 2 20415 49 x 2 98 7 714 5 510 now consider panel b of figure 1 it helps to clarify the differences between the douglass et comparison of model and observed trends and our own comparison the black horizontal line multimodel mean trend is the t2lt trend in the 19model ensemble calculated from model ensemble mean trends the colored symbols douglass et als consistency criterion sigmase is given by sigmase sigma sqrtn 1 where sigma is the standard deviation of the 19 ensemblemean trends and n is 19 the orange and yellow envelopes denote the 1 and 2sigmase regions douglass et use sigmase to decide whether the multimodel mean trend is consistent with either of the observed trends they conclude that the rss and uah trends lie outside of the yellow envelope the 2sigmase region and interpret this as evidence of fundamental inconsistency between modeled and observed trends as noted previously douglass et obtain this result because they fail to account for statistical uncertainty in the estimation of the rss and uah trends they ignore the statistical error bars on the rss and uah trends which are shown in panel as is clear from panel the statistical error bars on the rss and uah trends overlap with the douglass et 2sigmase region had douglass et accounted for statistical uncertainty in estimation of the observed trends they would have been unable to conclude that all uah and rss satellite trends are inconsistent with model trends the second figure plots values of our test statistic d1 for the paired trend test the grey histogram is based on the values of d1 for the 49 tests involving the rss t2lt trend and the simulated t2lt trends from 20c3m runs the green histogram is for the 49 paired trend tests involving model 20c3m data and the uah t2lt trend note that the d1 distribution obtained with the uah data is negatively skewed this is because the numerator of the d1 test statistic is bo bm and the uah tropical t2lt trend over 19791999 is smaller than most of the model trends see figure 1 panel the colored dots are values of the d1 test statistic for what i referred to previously as type2 tests these tests are limited to the m models with multiple realizations of the 20c3m experiment here m 11 for each of these m models i performed paired trend tests for all c unique combinations of trends pairs for example for model with 5 realizations of the 20c3m experiment like gisseh c 10 the significance of trend differences is solely function of within model effects ie is related to the different manifestations of natural internal variability superimposed on the underlying forced response there are total of 62 paired trend tests note that the separation of the colored symbols on the yaxis is for visual display purposes only and facilitates the identification of results for individual models the clear message from figure 2 is that the values of d1 arising from internal variability alone are typically as large as the d1 values obtained by testing model trends against observational data the two negative outlier values of d1 for the modelversusobserved trend tests involve the large positive trend in cccmacgcm31t47 if you have keen eagle eyes youll note that the distribution of colored symbols is slightly skewed to the negative side if you look at panel of figure 1 youll see that this skewness arises from the relatively small ensemble sizes consider results for the 5member ensemble of 20c3m trends from the mricgcm232 the trend in realization 1 is close to zero trends in realizations 2 3 4 and 5 are large positive and vary between 027 to 037 degrees cdecade so d1 is markedly negative for tests involving realization 1 versus realizations 2 3 4 and 5 if we showed nonunique combinations of trend pairs eg realization 2 versus realization 1 as well as 1 versus 2 the distribution of colored symbols would be symmetric but i was concerned that we might be accused of double counting if we did this the third figure is the most interesting one you have not seen this yet i decided to examine how the douglass et consistency test behaves with synthetic data i did this as function of sample size n for n values ranging from 19 the number of models we used in the ccsp report to 100 consider the n 19 case first i generated 19 synthetic time series using an ar1 model of the form xti a1 xti1 am zti am where a1 is the coefficient of the ar1 model zti is randomlygenerated noise term and am is mean set to zero here here i set a1 to 086 close to the lag1 autocorrelation of the uah t2lt anomaly data the other free parameter is scaling term which controls the amplitude of zti i chose this scaling term to yield temporal standard deviation of xti that was close to the temporal standard deviation of the monthlymean uah t2lt anomaly data the synthetic time series had the same length as the observational and model data 252 months and monthlymean anomalies were calculated in the same way as we did for observations and models for each of these 19 synthetic time series i first calculated leastsquares linear trends and adjusted standard errors and then performed the paired trends the test involves all 171 unique pairs of trends b1 versus b2 b1 versus b3 b1 versus b19 b2 versus b3 etc i then calculate the rejection rates of the null hypothesis of significant difference in trend for stipulated significance levels of 5 10 and 20 this procedure is repeated 1000 times with 1000 different realizations of 19 synthetic time series we can therefore build up distribution of rejection rates for n 19 and then do the same for n 20 etc the paired trend results are plotted as the blue lines in figure 3 encouragingly the percentage rejections of the null hypothesis are close to the theoretical expectations the 5 significance tests yield rejection rate of little over 6 10 tests have rejection rate of over 11 and 20 tests have rejection rate of 21 im not quite sure why this slight positive bias arises this bias does show some small sensitivity 12 to choice of the a1 parameter and the scaling term different choices of these parameters can give rejection rates that are closer to the theoretical expectation but my parameter choices for the ar1 model were guided by the goal of generating synthetic data with roughly the same autocorrelation and variance properties as the uah data and not by desire to get as close as i possibly could to the theoretical rejection rates so why is there small positive bias in the empirically determined rejection rates perhaps francis can provide us with some guidance here karl believes that the answer may be partly linked to the skewness of the empiricallydetermined rejection rate distributions for example for the n 19 case and for 5 tests values of rejection rates in the 1000member distribution range from minimum of 0 to maximum of 24 with mean value of 67 and median of 64 clearly the minimum value is bounded by zero but the maximum is not bounded and in rare cases rejection rates can be quite large and influences the mean this inherent skewness must make some contribution to the small positive bias in rejection rates in the paired trends test what happens if we naively perform the paired trends test without adjusting the standard errors of the trends for temporal autocorrelation effects results are shown by the black lines in figure 3 if we ignore temporal autocorrelation we get the wrong answer rejection rates for 5 tests are 60 we did not publish results from any of these synthetic data experiments in our 2000 jgr paper in retrospect this is bit of shame since figure 3 nicely shows that the adjustment for temporal autocorrelation effects works reasonably well while failure to adjust yields completely erroneous results now consider the red lines in figure 3 these are the results of applying the douglass et consistency test to synthetic data again lets consider the n 19 case first i calculate the trends in all 19 synthetic time series lets consider the first of these 19 time series as the surrogate observations the trend in this time series b1 is compared with the mean trend bsynth computed from the remaining 18 synthetic time series the douglass sigmase is also computed from these 18 remaining trends we then form test statistic d2 b1 bsynth sigmase and calculate rejection rates for the null hypothesis of significant difference between the mean trend and the trend in the surrogate observations this procedure is then repeated with the trend in time series 2 as the surrogate observations and bsynth and sigmase calculated from time series 1 3 419 this yields 19 different tests of the null hypothesis repeat 1000 times and build up distribution of rejection rates as in the paired trends test the results are truly alarming application of the douglass et consistency test to synthetic data data generated with the same underlying ar1 model leads to rejection of the abovestated null hypothesis at least 65 of the time for n 19 5 significance tests as expected rejection rates for the douglass consistency test rise as n increases for n 100 rejection rates for 5 tests are nearly 85 as my colleague jim boyle succinctly put it when he looked at these results this is pretty hard test to pass i think this nicely illustrates the problems with the statistical approach used by douglass et if you want to demonstrate that modeled and observed temperature trends are fundamentally inconsistent you devise fundamentally flawed test is very difficult to pass i hope to have first draft of this stuff written up by the end of next week if leo is agreeable figure 4 of this grl paper would show the vertical profiles of tropical temperature trends in the various versions of the raobcore data plus model results sorry to bore you with all the gory details but as weve seen from douglass et details matter with best regards ben benjamin d santer program for climate model diagnosis and intercomparison lawrence livermore national laboratory po box 808 mail stop l103 livermore ca 94550 usa tel 925 4222486 fax 925 4227675 email santer1llnlgov