wikileaks document release httpwikileaksorgwikicrsrl33301 february 2 2009 congressional research service report rl33301 congress and program evaluation an overview of randomized controlled trials rcts and related issues clinton t brass government and finance division blas nunezneto and erin d williams domestic social policy division march 7 2006 abstract before discussing rcts in detail this report places them in context by discussing 1 questions that program evaluations are typically intended to address 2 how rcts relate to other program evaluation methods and 3 two major roles that congress often takes with regard to program evaluation the report next describes the basic attributes of an rct major ways to judge an rcts quality and diverse views about the practical capabilities and limitations of rcts as form of program evaluation in light of concerns about the reliability of individual studies to support decision making the report also discusses how rcts can ﬁt into systematic reviews of many evaluations the report next highlights two areas where rcts have garnered recent attention in education policy and the presidents annual budget proposal to congress finally the report identiﬁes potential issues for congress that could apply to the highlighted cases oversight of other policy areas and pending legislation because the vocabulary of program evaluation can be confusing an appendix provides glossary with deﬁnitions of selected termshttpwikileaksorgwikicrsrl33301congressional research service the library of congresscrs report for congress received through the crs weborder code rl33301 congress and program evaluation an overview of randomized controlled trials rcts and related issues march 7 2006 clinton t brass analyst in american national government government and finance division blas nunezneto analyst in social legislation domestic social policy division erin d williams specialist in bioethical policy domestic social policy divisionhttpwikileaksorgwikicrsrl33301congress and program evaluation an overview of randomized controlled trials rcts and related issues summary program evaluations can play an importa nt role in public policy debates and in oversight of government programs poten tially affecting deci sions about program design operation and funding one tec hnique that has r eceived significant recent attention is the randomized controlled trial rct there are also many other types of evaluation including observa tional and qualitative designs an rct attempts to estimate progra ms impact upon an outcome of interest eg crime rate an rct randomly ssigns subjects to treatment and control groups administers an intervention to th treatment group and afterward measures the average difference between the groups the quality of an rct is typically assessed by its internal external and construct validity at the federal level rcts have been subject of interest and some controversy in edu cation policy and the george w bush administrations effort to integrate budgeting and performance using the program assessment rating t ool part in addition in the 109th congress pending legislati on provides for rcts eg s ections 3 and 15 of s 1934 section 114 of s 667 senate committeerepor ted bill and sec tion 5 of s 1129 views about the practical capabilities and limitations of rcts compared to other evaluation designs have sometimes been contentious there is wide consensus that under certain conditions welldesigned and imple mented rcts provide the most valid estimate of an interventions impact and can therefore provide useful information on whether and the extent to which an intervention causes favorable impacts for large group of subjects on aver age however rcts are also seen as difficult to design and implement well th ere also appears to be less consensus about what proportion of evaluations that are intended to estimate impacts should be rcts and about the conditions under which rcts ar appropriate many observers argue that other types of evaluations are necessary complements to rcts or sometimes necessary substitutes for them and can be used to establish causation help bolster or undermine an rcts findings or in so situations valid ly estimate impacts there is increasing consensus that single study of any type is rarely sufficient to reliably support decision making many researchers have therefore embraced systematic reviews which synthesize many similar or disparate studies number of issues regarding rcts mig ht arise when congress considers making program evaluation policy or when actors in the policy process present program evaluations to influence congress should congress focus on rcts in these situations number of issues might be c onsidered including an rcts parameters capabilities and limitations in addition congress might examine the types of program evaluations that are necessary question an evaluations definitions or assumptions consider how to appropriately use evaluation information in its learning and decision making evaluate how much confidence to have in study and investigate whether agencies have capacity to properl conduct interpret and objectively present evaluations th is report will be updated in the 110th congresshttpwikileaksorgwikicrsrl33301contents introduction 1 congress program eval uation and policy making 1 key questions about governme nt programs and policies 1 p r g r m v l u t i n n d i n f r m d p l i c m k i n g 2 t p s f p r g r m v l u t i n3 r n d m i z d c n t r l l d t r i l s r c t s 4 many other methods 5 p s s i b l c n g r s s i n l r l s c n c r n i n g p r g r m v l u t i n 5 m k i n g p r g r m v l u t i n p l i c y5 s c r u t i n i z i n g n d l r n i n g f r m p r g r m v l u t i n s 6 r n d m i z d c n t r l l d t r i l s r c t s 7 w h t r r c t s 7 r c t d f i n d 7 i n t r n l v l i d i t 8 x t r n l v l i d i t 9 c n s t r u c t v l i d i t 1 0 evaluation quality 1 1 practical capabilities and limitations of rcts 1 1 rct capabilities 1 3 rct limitations 1 5 r c t s i n c n t x t p r g r m v l u t i n n d s s t m t i c r v i w1 7 concerns about single studies and study quality 1 7 study quality hierarchy of evidence 1 7 s s t m t i c r v i w i n h l t h c r e1 8 s s t m t i c r v i w i n s c i l s c i n c r l t d r s2 0 recent attention to using rcts in program evaluation 2 1 c n t r v r s i n d u c t i n p l i c p r i r i t f r r c t s 2 1 authority cited for the ed priority nclb and s c i n t i f i c l l b s d r s r c h 2 1 r c t i n s t t h p r i r i t y2 3 i m p l i c t i n s n d r l t d d v l p m n t s 2 4 s s s s i n g p r g r m s i n t h b u d g t p r c s s t h p r t 2 7 the bush administrations program assessment rating t l p r t 2 7 u s f t h p r t 2 9 r c t s n d t h p r t 3 0 j u d g i n g s u c c s s 3 3 p t n t i l i s s u s f r c n g r s s 3 6 i s s u s w h n d i r c t i n g r s c r u t i n i z i n g r c t s 3 6 c n s i d r i n g s t u d p r m t r s3 6 scrutinizing or prospectivel assessing studies internal n d x t r n l v l i d i t 4 0 i s s u s w h n d i r c t i n g r s c r u t i n i z i n g p r g r m v l u t i n s 4 2 w h t t p s f v l u t i n s r n c s s r 4 3httpwikileaksorgwikicrsrl33301w h t d f i n i t i n s n d s s u m p t i n s r b i n g u s d 4 3 how should congress use eval uation information when c n s i d r i n g n d m k i n g p l i c 4 4 how much confidence should one have in study in order t i n f r m n s t h i n k i n g n d d c i s i n s 4 6 do agencies have capacity and independence to properly conduct interpret and ob jectively present program v l u t i n s 4 7 p p n d i x g l s s r f s l c t d t r m s n d c n c p t s 5 0 t h v c b u l r f p r g r m v l u t i n 5 0 s l c t d t r m s n d c n c p t s 5 0httpwikileaksorgwikicrsrl33301 congress and program evaluation an overview of randomized controlled trials rcts and related issues introduction program evaluations can play an importa nt role in public policy debates and in oversight of government programs poten tially affecting deci sions about program design operation and funding many diffe rent techniques of program evaluation can be used and presented with an intention to inform and influence policy makers one technique that has received significant recen t attention in the federal government is the randomized controlled trial rct this report discusses what rcts are and identifies number of issues regarding rcts that might arise when congress considers making program evaluation policy for example in the 109th congress section 3 of s 1934 as introduced woul d establish priority for rcts when evaluating offender reentry demonstration projects s ection 114 of s 667 senate finance committeereported bill would require rcts for demonstration projects for lowincome families and s ection 5 of s 1129 as introduced would call for rcts for projects and policies of multilateral development banks issues regarding rcts could also arise when actors in the policy process present specific program evaluations to congress eg in the pr esidents budget proposals to influence congresss views and decision making fo r many reasons evaluations often merit scrutiny and care in interpretation before discussing rcts in detail the report places them in context by discussing 1 questions that program eval uations are typically intended to address 2 how rcts relate to other program ev aluation methods and 3 two major roles that congress often takes with regard to program evaluation the report next describes the basic attributes of an rct major ways to judge an rcts quality and diverse views about the practical capabilitie s and limitations of rcts as form of program evaluation in light of concerns about the reliability of individual studies to support decision making the report also discusses how rcts can fit into systematic reviews of many evaluations the report next highlights two areas where rcts have garnered recent attention in education policy and the presidents annual budget proposal to congress finally the report identifies potential issues for congress that could apply to the highlight ed cases oversight of other policy areas and pending legislation because the vocabulary of program evaluation can be confusing an appendix provides glossary with definitions of selected terms congress program evaluation and policy making key questions about govern ment programs and policies citizens elected officials civil servants interest groups and many other participants in governance of the united states have an interest in the performance and results ofhttpwikileaksorgwikicrsrl33301crs2 1 in program evaluation the terms intervention and treatment are sometimes used as continuedgovernment programs and policies to th at end stakeholders might want answers to many questions about programs and policies for exam ple how should public policy problems be defined is pr ogram addressing some or all of the problems how well are federal programs and policies managed what are they achieving how can they improve how are stakeholders affected what unintended consequences might result in the future what activities and policies should the federal government pursue in order to best serve the public what resources should be devoted to program or policy in addition stakeholders might want an swers to questions about the quality of evaluations that are brought to policy discu ssions given that participants in the policy process will not always adve rtise weaknesses in studies that also happen to support their policy positions what might those weaknesses be stak eholders might also ask how well federal agencies evaluate the programs they lead and administer for example what methods are appropriate to assess given type of program or policy given the available quantity and quality of research what degree of confidence should be placed in findings to date do agencies have sufficient capacity to evaluate their programs are they performing the necessary types of evaluation do agencies have sufficient independence to credibly evaluate their programs and policies what role should agenci play in evaluating programs at times many or all of these questions might be of interest to congress and program stakeholders all of them will typically be of interest to agency program managers and leaders therefore any of these questions might be potential subjects of congressional oversig ht or law making program evaluation and informed policy making in response to questions like those posed above program ev aluations can be introduced into policy discussions by actors in the policymaking process these actors who include organizations and individuals both inside and outside of government might be interest groups think tanks acad emics legislators state or local governments the president federal agencies or nonpartisan institutions many actors bring evaluations to policy discussions on their own initiative oftentimes to emphasize the results or findings that they interpret to support th eir positions some actors eg federal agencies might bring evaluations in re sponse to legislative or executive branch requirements depending on ma ny circumstances the evaluations that agencies bring might or might not support the policy views of the agencys head or the president when actors bring program evaluations in to policy discussions the studies will oftentimes use different approaches because there are many possible ways to help answer the questions cited previously the term program evaluation therefore has in practice been interpreted in several ways for example there is consensus definition for the term program in practice the term has been used to refer to government policy activity project initia tive law tax provi sion function or set thereof accordingly this report uses the term program to refer to any of these things as appropriate that someone might wish to evaluate1 the term evaluationhttpwikileaksorgwikicrsrl33301crs3 1 continued synonyms for program 2 deborah m fournier evalu ation in sandra mathison ed encyclopedia of evaluation thousand oaks ca sage publications 2005 p 139 3 pl 10362 107 stat 285 at 288 the senate committee on governmental affairs report that accompanied gpra also clarified that th term should be read to include evaluations of unintended results program implemen tation and operating policies and practices but not routine program monitoring see us congress senate committee on governmental affairs government performance and results act of 1993 report to accompany s 20 103rd cong 1st sess june 16 1993 srept 10358 washington gp 1993 pp 3233 4 ibid p 30 see also section 4b of the act which was codified as 31 usc 1115bcan seem similarly ambiguous recent re ference work in the program evaluation literature defined evaluation as an applied inquiry process for collecting and synthesizing evidence that culminates in conc lusions about the state of affairs value merit worth significance or quality of program product person policy proposal or plan2 perhaps with many of these cons iderations in mind congress defined program evaluation for purposes of the governme nt performance and results act of 1993 gpra as an assessment thr ough objective measurement and systematic analysis of the manner and extent to which federal programs achieve intended objectives3 gpra requires most executive branch agencies to develop fiveyear strategic plans annual performance pl ans including goals and performance indicators among other things and annual program performance reports when reporting gpra to the senate the senate committee on governmental affairs contemplated that not all forms of eval uation and measurement would necessarily be quantifiable because of the divers ity of federal g overnment activities4 i n sum program evaluation has been considered in practice in the scholarly literature and under gpra as concerned with investig ating both program s operations and its results furthermore program evaluation s been seen as 1 informing conclusions at particular points in time and also 2 cumulative process over time of forming conclusions as more evaluation inform ation is collected and interpreted program evaluations might help inform policy makers including members and committees of congress in their authorizations appropriations and oversight work however viewpoints about program evalua tions can be contentious both in policy debates and among expert evaluators in their interactions with congress many actors cite program evaluations as part of the rationale for policy changes in addition observers and pr actitioners sometimes disa gree about the practical capabilities and limitations of va rious program evaluation methods the quality of an individual evaluation or how studys fi ndings should be interpreted and used therefore many observers believe it is important for policy makers including members and committees of congress to be informed consumers of evaluation information when weighing these consid erations and making policy decisions types of program evaluation practitioners and theorists categorize different types of program evaluation sometimes referred to as different designs or thods in several wayshttpwikileaksorgwikicrsrl33301crs4 5 large array of publications are available concerning different types of program evaluation for perspectives on different type s of evaluations and the purposes for which they can be used see for example us government accountability office performance measurement and evaluation definitions and relationships gao05739sp may 2005 joseph s wholey harry p hatry and kathryn newcomer eds handbook of practical program evaluation san francisco josseybass 1994 joseph s wholey harry p hatry and kathryn newcomer eds handbook of practical program evaluation 2nd ed san francisco josseybass 2004 peter h rossi mark w lipsey and howard freeman evaluation systematic approach 7th ed thousand oaks ca sage publications 2004 lawrence b mohr impact analysis for program evaluation 2nd ed thousand oaks ca sage publications 1995 william r shadish thomas d cook and donald t campbell experimental and quasiexpe rimental designs for gene ralized causal inference boston houghton mifflin 2001 nd michael quinn patton qualitative research and evaluation methods 3rd ed thousand oaks ca sage publications 2002 6 for example if the unemployment rate in geographic area would have been 6 without program intervention but was estimated to be 5 because of the intervention the impact would be 1 reduction in the unemployment ra ie 6 minus 5 equals an impact of 1 or alternatively as 167 reduction in the unemployment rate if one characterizes the impact as proportion of the prior unemplo yment rate some theorists and practitioners use the term effect as synonym for impact this report uses only the term impact for this definition to avoid potential confusion sometimes associated with the term effect unfortunately these categorizations are not always consistent with each other and practitioners and theorists do not always use consistent terminology to describe program evaluations they sometimes use different definitions for the same term or use different terms as synonyms for th same definition this report discusses some of these methods but does not atte mpt to provide an overall taxonomy for program evaluation types5 the sections below provide basic descriptions of rcts and other methods more detailed descripti on and discussion of rcts is located later in the report randomized controlled trials rcts one program evaluation method that has been subject of recent interest at the federal level as well as some controversy is the rct as discussed later in this report an rct attempts to estimate programs impact on an outcome of interest an outcome of interest is something oftentimes public policy goal that one or more stakeholders care about eg unemployment rate which many actors might like to be lower an impact is an estimated measurement of how an intervention affected the outcome of interest compared to what would have happened without the intervention6 simple rct randomly assigns some subjects to one or more treatment groups also sometimes called experimental or intervention groups and others to control group the treatment group participates in the prog ram being evaluated and the control group does not after the treatment group experi ences the intervention an rct compares what happens to the two groups by meas uring the difference between the two groups on the outcome of interest this difference is considered an estimate of the programs impact the terms randomized field trial rft random assignment design experimental design random experiment and social experiment are sometimes used as synonyms for rct and vi ce versa however use of the word field in this context is often intended to imp ly that an evaluation is being conducted in more naturalistic setting instead of laboratory or other artificial environmenthttpwikileaksorgwikicrsrl33301crs5 7 the term performance measurement can mean many things but is usually considered different from program evaluation frequently performance measurement refers to ongoing and periodic monitoring and reporting of pr ogram operations or accomplishments eg progress toward quantitative goals and sometimes also statistical information related to but not necessarily influenceable by program 8 congress sets program evaluation policy both for the executive branch generally eg government performance and results act of 1993 107 stat 285 and in specific policy areas eg education sciences reform act of 2002 116 stat 1940 congress has also set policy to enhance congresss own institutional capacity and the capacity of its supporting continuedmany other methods many other types of pr ogram evaluation that are not rcts can also be conducted in order to ddress one or more of the questions posed at the beginning of this report some of these other methods have been called as group observational designs the term observational design has been used in different ways but often refers to empirical and qualitative studies of many types that are intended to help explain causeandeff ect relationships but that do not attempt to approximate an ex perimental design quasiexperimental designs refer to studies that attempt to estimate treatments impact on group of subjects but in contrast with rcts do not have random assignment to treatment and control groups some quasiexperiments are controlled studies i with control group and at least one treatment group but others lack cont rol group so quasiexperiments do not measure the outcome of interest before the treatment takes place many observers and practitioners consider quasiexperiments to be form of observational design but others put them in their own categor methods that attempt to estimate an impact are sometimes called impact analysis designs qualitative evaluation often refers to judging the effectiveness of program eg whether it accomplishes its goals by conducting openended interv iews directly observing program implementation and outcomes reviewing documents and constructing case studies as used by different researchers the term nonexperiment has been used at times to refer specifically to quasiexperiments and other times to anything that is not an rct systematic reviews synthesize the results of ma ny studies as discussed later in this report many other program eval uation methods including surveys and cost benefit analyses are also used to assess programs7 because this report focuses on rcts and related issues only some of these other met hods are discussed further in this report possible congressional roles concerning program evaluation congress can assume at least two major roles regarding program evaluation these roles might be called 1 making program evaluation policy and 2 when presented with one or more program eval uations scrutinizing and learning from program evaluations each of these broa d roles can raise number of issues for congress regarding program evaluations generally as well as rcts specifically making program evaluation policy first congress might make policy regarding how when and the extent to wh ich agencies are to conduct fund or use program evaluations8 for example congress might among other things establishhttpwikileaksorgwikicrsrl33301crs6 8 continued agencies to evaluate policy and consider program evaluations eg legislative reorganization act of 1970 84 stat 1140 congressional budget act of 1974 88 stat 297 9 saville kushner program evaluation in sandra mathison ed encyclopedia of evaluation p 337 10 see michael scriven evaluation thesaurus 4th ed newbury park ca sage publications 1991 pp 227228 382383agencies or offices that have missions to evaluate programs require that percentage of programs funding be devoted to program evaluation activ ities appropr iate funds for specific evaluations artic ulate what questions should be studied or specify what methods should or must be used when policy makers consider and make these decisions two considerations that many observers would likely consider important are for policy makers to be aware of both the practical capabilities and limitations of various program evaluation methods nd also how those capabilities and limitations might be balanced in light of multiple evaluation objectives scrutinizing and learning from program evaluations second members of congress might use specific program evaluations to help inform their thinking policy making and oversight of federal policies in the course of congresss lawmaking and oversight work actors inside and out side of government frequently cite program evaluations to justify their policy proposals and recommendations in these situations consumers of evaluation information including congress can face challenges of assessing 1 quality and depth of evaluation information which can be uneve n and 2 the relevance of evaluation information to policy problem which can vary therefore should congress wish to critically assess or scrutinize program evaluations having insight into how to assess the quality depth and relevance of evaluation thods might be helpful program evaluations themselves can help inform policy in several ways among other things they can provide d eeper understanding of policy problem suggest possible ways to modify or improve program or policy provide perspectives on whether goals are being accomplished reveal consequences that might not have been intended and inform deliberations regarding the allocation of scarce resources nonetheless observers and stakeholders frequently disagree on the appropriate goals of government activities which can make evaluations controversial furthermore because pro gram evaluation is site for the resolution of ethical and democratic dilemmas9 any assessment of programs merit or worth is arguably always made in part through the lens of an observers priorities beliefs values and ethics merit and worth are program evaluation terms that often are defined as the overall intrinsic and extrinsic value respectively of program to individuals and society10 even when there is some consensus on goals it has sometimes been difficult or impossible to specify with single number program evaluation performance measur or even group of evaluations and performance measures how to comprehensively judge an organizations or programs success in accomplishing its mission thus as experien ce has shown the concepts of merit and worth are often in the eye of the beholder still evaluations might help clarify whathttpwikileaksorgwikicrsrl33301crs7 11 in an rct the units of analysis are typically individual persons but sometimes units might be things or organizations like schools hospitals or police stations 12 the concept that different subjects might respond differently to treatment and receive different impacts as opposed to assuming that everyone receives the same impact has sometimes been referred to as heterogeneous treatment impacts 13 randomly assigning subjects to an intervention group and comparison group increases confidence that on average the two groups are initially comparable random assignment also allows the use of certain statistical techniques for validly estimating an impact however sometimes the randomization is unlucky and does not necessarily result in comparable groups the statistical techniques attempt to account for this chance the term random assignment is different from the term random selection random assignment refers to assigning subjects to different groups or treatments in controlled study random selection refers to how one draws sample from larger population eg to undertake survey that is intended to be representative of broader populationprograms are accomplishing and when ev aluations are done well help policy makers make informed judgments when reconciling diverse views about policy problems and values randomized controlled trials rcts what are rcts rct defined as briefly noted earlier an rct is type of program evaluation that seeks to assess whether program had an impact for one or more outcomes of interest eg number of weeks person remains unemployed compared to what would have happened without the program an impact is usually calculated for large sample of subjects as the difference between 1 measurement of the outcome of interest after an intervention takes place averaged across subjects who received the treatment and 2 measurement of the outcome of interest after the intervention averaged across subjects who did not receive the treatment the study randomly assigns the subjects also called units of analysis 11 to one or more treatment groups and also control gr oup treatment gr oup experiences the intervention and the control group does not after the intervention measurement of the outcome of interest for the treatme nt group provides information on how the intervention might have affected these s ubjects the control group by contrast is intended to simulate what would have happened to th treatment group subjects if they had not received the interventi on depending on the policy area studied an intervention could be for example trai ning regimen for unemployed workers or new policy to reduce crime because the estimated impact is an average across subjects the impact reflects the weighted average of the subjects who experienced favorable impacts subjects who did not experience change and others who experienced unfavorable impacts12 in theory random assignment helps ensure that all of the groups in the study are made statistically equivalent at the beginning of the study13 if the only important difference in the subseque nt experience of each group is the intervention thenhttpwikileaksorgwikicrsrl33301crs8 14 the term statistical significance has different meanings in different contexts even though each meaning is based on the same statistical c oncepts in the context of an rct finding of statistical significance is typically interpre ted as level of confidence usually expressed as probability eg 95 which is also referred to as significance at the 05 level that an impact is not merely the result of random variation assuming the rct suffered from defects this finding would indicate that at least some of the measured impact may with substantial confidence eg 95 confidence be attributed to the treatment as cause stated another way significance at the 05 level indicates that there 1 in 20 chance that the observed difference could have occurred by chance if program actually had impact however simply because an estimated impact is found to be statistically significant does not necessarily mean the impact is large or important see lawrence b mohr impact analysis for program evaluation pp 124 127130 see also thomas h wonnacott and ronald j wonnacott introductory statistics for business and economics 4th ed new york wiley 1990 ch 9 15 thomas d cook and donald t campbell quasiexperimentati on design and analysis issues for field settings chicago rand mcnally 1979 ma rilynn b brewer internal validity in michael s lewisbeck an bryman and tim futing liao eds the sage enclyclopedia of social science research methods thousand oaks ca sage 2004 p 502differences in the outcome of interest that are observed at the end of the trial can be attributed with greater confidence to the in tervention rather than to initial differences between the groups various statistical tools can be used to estimate whether observed differences are likely due to the intervention ie the difference is found to be statistically significant with small chance of error or to chance14 the quality of an rct is ofte n assessed by two criteria internal validity and external validity third criterion construct validity is not always discussed but is also considered important for judging an evaluations quality internal validity internal validity is typically defined as the confidence with which one can state that the impact found or implied by study was caused by the intervention being studied for example if an rct of an aftercare program for juveniles shows that the juveniles who attended program recommit crimes recidivate at lower rate than the j uveniles who did not ttend the program the control group an assessment of the studys internal validity would suggest whether this result was due to the aftercare prog ram or whether it might have been due to some other factor internal validity is pr edicated on the methodol ogical rigor of the study and an absence of other factors unrelated to the program affecting the outcome of interest for either the treatment or control group di fferently from the other group the term also reflects that the better designed and implemented study is the more reliable its conclusions about causation will tend to be15 from the perspective of internal validity the methodological rigo r of an rct study can depend on number of factors including but not limited to the following how effectively the random assignme nt of units creates statistically equivalent groups whether the group of subjects is fficiently numerous to ensure that an impact large enough to be of inte rest to stakeholders if it occurs will be found statistica lly significant the more numerous the unitshttpwikileaksorgwikicrsrl33301crs9 16 see lawrence b mohr impact analysis for program evaluation pp 9297in the group being studied the bette r chance there is of detecting programs potential impact whether attrition in the treatment and control groups eg subjects dropping out of the study is comparable with respect to the attributes of subjects whether factors other than the treatment contaminate one group but not the other eg due to pr oblems with delivering the treatment or incomplete environmental controls whether the behavior of researchers or subjects is affected becausethey know who is receiving trea tment or not receiving treatment ideally rcts are doubleblind studies in which neither the subjects nor the researchers know which group gets the treatment but doubleblind studies in so cial science are uncommon whether the units being studied comply with the intervention being provided eg did the patient take the medicine being studied whether the presence of randomized evaluation influences the treatment groups experience eg randomization altering the process of selection into the program whether subjects in the control gr oup can obtain close substitutes for the treatment outside the program and whether data collection and anal ysis procedures are reliable external validity external validity is typically defined as the extent to which an intervention being studied can be applied to other ttings times or groups of subjects and be expected to deliver similar impact on an outcome of interest16 thus external validity relates to both 1 whether the intervention itself can bereplicated with high confidence and 2 whether an intervention will most likely result in similar impact in other situations or environments or with other subjects in practice some users of the term empha size the second aspect noted here the terms generalizability replicability and repeatability are sometimes used as synonyms for external validity the external validity of study can pend on variety of factors as noted above one factor is the confidence person has that an intervention itself can be replicated for example if new curriculum is introduced in school it is possible that the school might deviate from th prescribed curriculum in order to accommodate events or student needs unanticipated by the designers of the new curriculum or the study researchers unless the deviation were clearly documented it might be difficult or impossible to replicate the same intervention in other sites furthermore if the deviation affects the studys outcome of interest either positively or negatively compared to what it would ve been without the deviation the studys findings might not be generalizable another major factor relating to external validity and one of the most frequently cited is the way in which studys subjects were selected for example the results of study measuring the impact of cer tain curriculum in schools in boston new york city and philadelphia might not be generalizable to classrooms in small townshttpwikileaksorgwikicrsrl33301crs10 17 construct validity in sandra mathison ed encyclopedia of evaluation p 81 18 for discussion see william mk trochim the research methods knowledge base 2nd ed cincinnati oh atomic dogpublishingcom 2001 p 69in the midwest due to potential differen ces among the underlying populations and environments the results also might not be generalizable to classrooms in other large cities among rcts the most generalizable are often those that estimate the same interventions impact in different settings also known as multisite rcts as well as those which feature samples of subjects with diverse socioeconomic and demographic characteristics rcts can also be more generalizable if subjects are randomly selected from certain population to participate in the rct in order to make the subjects more representative of that population this is not always possible however because sometimes subject s can be selected only in nonrandom way eg if subjects volunteer for the prog ram attrition can also affect external validity even if attrition am ong the treatment and control groups is equivalent if too many people with certain characteristics drop out of study treatment group that was diverse enough to provide some gene ralizability at the beginning of the study may longer have as much at the end ev if the sample re mains large enough to produce statistically significant results concerning the interventions impact in response to many of these consid erations researcher s sometimes carefully describe the intervention studys subjects and the local environment so that other researchers and stakeholders can attemp t to assess studys external validity construct validity third type of validity that is considered to be important in any type of evaluation but is not always explicitly discussed is construct validity as one reference work explains simplistically construct validity is about naming something program an attribute accurately for example when an evaluator measures student achievement the issue of construct validity entails judgment about whether the measures or the operationalization of ac hievement are really measuring student achievement as opposed to for example social capital17 the construct in this example is specific way of measuring student achievement thus one definition of cons truct validity concerns the extent to which study actually evaluates the question it is being represented as evaluating perhaps unsurprisingly actors in the policy process will sometimes have different views on appropriate ways to measure student ach ievement or more generally appropriate ways to measure success in achievi ng programs mission and goals alternatively when the term construct validity is used in relation to program rather than measurement method it often re fers to the extent to which an actual program reflects ones ideas and theories of 1 how the program is supposed to operate and 2 the causal mechanism th rough which it is supposed to achieve outcomes18 this view of construct validity can be important when attempting to improve program eg modifying it to be tter achieve goals or to understand the circumstances that are necessary for the program to achieve similar results at anotherhttpwikileaksorgwikicrsrl33301crs11 19 lawrence b mohr impact analysis for program evaluation p 97 20 jesse berlin and drummond rennie measuring the quality of trials the quality of quality scales jama vol 282 sept 15 1999 pp 10831085time or place or with different subjects with insight into the mechanism of causation it might also be possible to mitigate any unintended consequences evaluation quality each type of validity is considered important to the overall quality of an rct high internal validity helps to ensure that estimated impacts were due to the intervention being studied and not to other factors such as contamination of the experiment eg improper treatment delivery or incomplete or improper environmental controls when th treatment and control groups experience different events aside from the treatment high external validity helps to ensure that an intervention could achieve similar results for other subjects at another time or in different setting high construct validity helps to increase confidence that 1 the outcome of interest actually measures what it is being represented as measuring and 2 the program actually caused an impact in the way that was theorized or intended however it is not necessarily always possible to enjoy the best of all of these worlds in an evaluation for example many scholars and practitioners have viewed rcts as an evaluation design that although potentially having high internal validity in certain cases can lack external validity eg if random assi gnment makes it more difficult to use typical subjects and natural or representative settings19 some have seen rcts as trading off external validity in order to achieve high internal validity but others disagree that there is an implied tradeoff rcts with low internal validity cannot be used to confidently state that an intervention caused an observed impact because the evidence they provide may be dubious rcts with high internal validity but low external validity may indicate that an intervention somehow made an impact for one population but not whether the inte rvention can be replicated and would make an impact in different population or tting with regard to construct validity there is not always consensus that particular outcome of interest represents the best way to evaluate program in addition establishing the mechanisms of causation eg to ensure the intervention caused an impact in the theorized way can be difficult complementary evaluation methods in addition to an rct might be required to do so commentators have so suggested other criteria for assessing quality for example some have concluded that even within rcts quality is an elusive metric and that in addition to internal validity complete definition of quality also should take into account the tr ials external validity and its statistical analysis as well as perhaps its ethical aspects20 practical capabilities and limitations of rcts claims about rcts practical capab ilities and limitations both in comparison with other research designs and in isol ation have at times been controversial although there is much regarding rcts bout which many observers agree certain issues have at times sparked controversy sorting out these arguments can be challenging however because the terms that observers use to describe rcts can be difficult to interpret for example some observers and practitioners view rcts ashttpwikileaksorgwikicrsrl33301crs12 21 frequent definition for effectiveness in program evaluation usually concerns achievement of desired and intended outcome but does not necessarily incorporate costs values or sometimes detrimental unintended outcomes see jane davidson effectivness in sandra mathison ed encyclopedia of evaluation p 122 some use effectiveness as synonym for merit and worth or less concretely success many researchers use the terms effect effective and effectiveness to refer or relate to impact the word effect is also sometimes used in the sense of something that inevitably follows an antecedent as cause or agent see merriamwebsters collegiate dictionary 11th ed springfield ma merriamwebster 2003 p 397 for example dropping pen cause results in noise when the pen hits th floor an effect in health care effectiveness has been defined as the benefit eg to health outcomes of using technology for particular problem under general or routine conditions whereas related term efficacy has been defined as the benefit of using technology for particular problem under ideal conditions for example in laboratory setting see national institutes of health national library of medicine glossary available at h ttpwwwnlmnihgovnichsrhta101ta101014html 22 see coalition for evidencebased policy bringing evidencedriven progress to education recommended strategy fo r the us department of education nov 2002 p 29 which called for rigorous study designs to be such prerequisite and characterized only rcts and certain quasiexperimental design s as rigorous the report is available at httpcoexgovsecuresitesnetadminformm anagerfilesuploadingcoalitionfinrptpdf 23 for an example of what appears to be qualitative method used to estimate an impact prospectively that appeared to influence decision making see michael moss pentagon study links fatalities to body armor new york times jan 7 2006 p a1 in response to disclosure of the study the senate and house committees on armed services reportedly said they would hold hearings on the matter see michael moss pentagon acts on body armor new york times jan 21 2006 p a6 and michael moss military says it is speeding efforts to add side armor new york times feb 2 2006 p a18the best way to determine programs effectiveness however it is not always clear whether these observers are using the term effectiveness as synonym for impact merit and worth or accomplishment of specific intended goals21 as illustrated later in this repor t in connection with educa tion policy and recent uses of program evaluation during the federal budget process apart from complications stemming from terminology some observers appear to have advocated that government soci and economic activ ities should be funded only if they can be proven effective by rcts and certain quasiexperiments22 others might dispute such emphasis on rcts or have different threshold for what proven means in addition some observers appear to see rcts and nonrcts ie any evaluation design other than an rct prima rily as competitors or substitutes for each other when judging effectiveness for example they might see value in rcts and quasiexperiments for their ability to estimate an impact but less in other evaluation methods that are not designed to estimate an impact observers might also see less value in other methods that do attempt to estimate an impact but that are judged to be so unreliable as to have little or value as noted in the next section other observers argue observational and qualita tive methods can be appropriate for estimating impacts in various circumstances23 at the same time many observers have seen rct and nonrct designs as comp lements rather than substitutes in somehttpwikileaksorgwikicrsrl33301crs13 24 for example see national research council richard j shavelson and lisa towne eds scientific research in education washington national academy press 2002 pp 108 109 and cesar g victora jeanpierre habich t and jennifer bryce evidencebased public health moving beyond randomized trials american journal of public health vol 94 mar 2004 pp 400405 when combined these are often called multiple or mixed methods evaluations 25 see portions of jennifer c greene and gary t henry qualitativequantitative debate in evaluation in sandra mathison ed encyclopedia of evaluation pp 345350 26 because the estimated impact is an average across many subjects it is possible the intervention may have affected different subjects in very different ways eg some positively some not at all and some negativel some scholars have therefore advocated using methods to look at more than just an average impact 27 for description of the four phases http wwwclini caltrialsgovctinfoglossary and 21 cfr 31221 for phases 1 through 3 and 31285 for phase 4 situations24 for example many observers ve argued that nonrct studies such as indepth case studies and other observational or qualitative methods are among other things 1 capable of casting doubt on an rcts findings or causal claims by showing the rct was contaminated or d poor design or imp lementation ie revealing poor internal validity 2 capab of showing studys inferences are flawed or questionable eg if the measured outcome of interest is judged to not fully reflect the programs goals raising questions of construct validity 3 essential for establishing the theory and conditions under which an intervention would be expected to make favorable impact inc reasing external validity and 4 capable of establishing or strongly suggesting causation in certain circumstances increasing internal validity even if study was not intended to estimate an impact nonetheless different observers have at times seen either quantitative or qualitative methods as misguided and the other met hods as preferable or more legitimate25 in light of the issues noted above am ong others several considerations about the practical capabilities and limitations of rcts are summarized below rct capabilities there is wide consensus that under certain conditions welldesigned and implemented rcts provide the most valid estimate of an interventions average impact for large sample of subjects as measured on an outcome of interest26 this is the reason for the often stated claim by some observers particularly in the medical field that welldesigned and implemented rcts that are also doubleblind are the gold standard for making causal inference about an interventions impact on an outcome of intere st usage of the term gold standard in fields other than medicine to desc ribe the value of rcts however has been considerably more contentious rcts have been extensively used for decades in the medical arena as usually the third of four phases of the process for helping the food and drug administration fda evaluate dr ugs devices and biological products for approval and for helping the medical community identify procedures that yield the most favorable health outcomes on selected outcomes of interest27 rigorously estimating an impact is highly valued b ecause it provides measurement of thehttpwikileaksorgwikicrsrl33301crs14 28 gary burtless the case for randomized field trials in economic and policy research journal of economic perspectives spring 1995 vol 9 2 pp 6384 29 see steven glazerman dan m levy nd david myers nonexperimental versus experimental estimates of earnings impacts annals of the american academy of political and social science 589 sept 2003 pp 6393 howard s bloom et can nonexperimental comparison group met hods match the findings from random assignment evaluation of mandatory welfaretowork programs mdrc working paper on research methodology june 20 02 available at http wwwmdrcorgpublications 66abstracthtml and thomas d cook william r shadish jr and vivian c wong within study comparisons of experiments nd nonexperiments ca n they help decide on evaluation policy paper presented at the french econometric society meeting on program evaluation paris france d ec 2005 available at http wwwcr estfr conferenceprogram_bishtm 30 nonetheless even in an experiment there is chance that an unlucky randomization of subjects could result in treatment and control groups that are not comparablemagnitude of programs imp act on one or more outcomes that stakeholder values and permits comparison among alternative treatments28 in contrast studies that rely on nonra ndom assignment betw een treatment and control groups eg quasiexp eriments or control group at all can be subject to certain threats to internal validity that undermine ones ability to make causal inference when estimating an average impact for large number of subjects29 for example if subjects are not randomly assign ed to treatment and control groups there is greater risk that prior existing difference between the two groups might be responsible for observed differences on an out come of interest after an intervention this threat to validity is often called selection bias 30 in view of these strengths and advantages among others there seems to be some consensus among program evaluation theorists and practitioners in variety of disciplines public policy analysis evaluation economics statistics etc that generally speaking more rcts should be performed in social sciencerela ted areas when appropriate as part of broad portfolio of evaluati on strategies and methods nonetheless there appears to be less consensus in variety of disciplines about what proportion of evaluations intended to estimate impacts should be rcts eg as opposed to quasiexperiments and other designs what proportion of evaluations overall should be rcts in light of di verse evaluation needs and the conditions under which rcts would be most valuable appropriate nd likely to result in valid findings for example many economists and other observers argue that rcts have an important and often preferred role to play in estimating impacts compared to quasiexperiments due to their high potential internal validity however they have also argued that certain types of quasie xperimental methods i those often called econometric methods are under certain conditions for each method capable of validly estimating impacts in ways that come reasonably close to the estimates from experimental methods they furthermor have argued that quasiexperimental methods can provide useful information if an rct is judged inappropriate due to factors like research needs program circumstances expense timing requirementshttpwikileaksorgwikicrsrl33301crs15 31 for discussion of some of these issues see jeffrey smith evaluating local economic development policies theory and practice in alistair nolan and ging wong evaluating local economic and employment development how to assess what works among programmes and policies paris oecd 2004 pp 287332 and robert moffitt the role of randomized field trial in social sc ience research perspective from evaluations of reforms of social welfare programs na tional bureau of economic research working paper t0295 oct 2003 32 one example of such critique taken from the literature about education research might be elizabeth adams st pierre science rejects postmodernism educational researcher vol 31 nov 2002 pp 2527 the term experiment is often associated with science however science is not only expe rimental science has been defined as the observation identification description expe rimental investigation and theoretical explanation of phenomena american heritage dicti onary of the english language 3rd ed boston houghtn miff lin 1992 p 1616 for discu ssion about the definition of science see crs report rl32992 the endangered species act and sound science by eugene h buck m lynne corn and pamela baldwin some observers make distinction often controversial one among scientists and scholars between hard science and soft science the natural sciences includi ng physics chemistry and many fields of biology have sometimes been called hard nd the social sciences including fields such as psychology sociology economics and political science have sometimes been called frequently pejoratively soft some scholars have made influential cr itiques of this distinction see thomas s kuhn the structure of scientific revolutions 2nd ed chicago university of chicago press 1970 in general phenomena in the natural sciences have tended to be seen as easier to observe quantify and experiment within controlled settings while phenomena in the social sciences have tended to be seen as more difficult to observe quantify and experiment within controlled settings 33 the question if an experiment will be implemented well has been considered big if according to some scholars see william mk trochim the research methods knowledge base p 191 34 james j heckman and jeffrey smith assessing the case for social experiments journal of economic perspectives spring 1995 pp 85110ethical considerations or other factors31 it should also be noted however that some academics and practitioners have perceived argume nts for rcts and other quantitatively oriented evaluation met hods as attempts to exclude some kinds of qualitative research from being considered s cientific or being funded or considered in policyoriented re search or debates32 rct limitations scholars and practitioners have also qualified what they view as practical capabilities of rcts particularly in areas of public policy that closely relate to the social sciences rcts are seen as very strong in making cause effect inferences about impacts for large samp of subjects if they are designed and implemented well however rcts are often seen as difficult to design and implement well33 number of observers argue that there is sizable divergence between the theoretical capabilities of eval uations based on random assignment and the practical results of such evaluations34 some policy areas have been seen as more difficult compared to others for successfully implementing rcts leading some observers to disavow the gold standa rd title for rcts while still supportinghttpwikileaksorgwikicrsrl33301crs16 35 see thomas d cook and monique r payne objecting to the objections to using random assignment in educational research in frederick mosteller and robert boruch eds evidence matters washington dc brookings in stitution press 2002 p 174 for example some scholars reject calling rcts the gold standard because among other things shortfalls often occur when implem enting experiments in the field and we do not yet know enough about the robustness of designs to withstand these reality bruises see thomas d cook william r shadish jr and vivian c wong within study comparisons of experiments and nonexperi ments can they help deci on evaluation policy p 31 36 see john concato nirav shah and ralph i horwitz randomized controlled trials observational studies and the hierarchy of research designs the new england journal of medicine vol 342 june 22 2000 pp 18871892 37 lawrence b mohr the causes of human behavior im plications for theory and method in the social sciences ann arbor university of michigan press 1996 pp 910 38 see lawrence b mohr impact analysis for program evaluation pp 9297 and national research council scientific research in education p 125 increased use of rcts as the most credible way to estimate an impact35 some researchers in the medical field have argued that certain types of welldesigned and implemented observational studies can yield similar results to rcts36 as with all forms of study poorly designed or imple mented rcts can yield inaccurate results in addition an rcts capability to suppor t causal inferences does not necessarily hold for determining causality in small numbers of subjects or individual cases for which other methods are ofte n judged more appropriate37 nor do rcts necessarily provide an advantage compared to other ev aluation research designs in generalizing specific interventions ability to make an impact to broader or different population rcts have therefore been seen by some observers as often having external validity limitations38 frequently rcts rely on support from other evaluation methods for making infe rences about external validity as discussed in more detail later in this report rcts can sometimes be seen as impractical unethical requiring too much time or being too costly compared to other designs that also seek to assess whether program causes favorable impacts and outcomes there is wide consensus th at rcts are particularly well suited for answering certain types of questions but not necessarily other questions compared to other evaluation research designs for example rcts typically do not assess how and why impacts occur how program mig ht be modified to improve program results or programs costeffectiveness rcts also typically do not provide full picture of whether unintende d consequences may have resulted from program or indicate whether study is us ing valid measures or concepts for judging programs success eg assessing studys or measur construct validity many of these kinds of questions have been considered to be more appropria tely addressed with observational or qualitative designs httpwikileaksorgwikicrsrl33301crs17 39 mike clark the cochrane collaboration systematic reviews and the cochrane collaboration apr 22 2004 available at http wwwcochraneorgdoc swhycchtm 40 some observers have argued against rig id hierarchy based on studies that found well designed observational studies to yield sim ilar results to rcts and that found rcts to sometimes offer conflicting results see john concato nirav shah and ralph i horwitz randomized controlled trials observationa l studies and the hi erarchy of research designs the new england jour nal of medicine june 22 2000rcts in context program evaluation and systematic review concerns about single studies and study quality in variety of research fields there appears to be cons ensus that single study matter how well designed or implemented is rarely sufficient to relia bly support decision making for example in health care where rcts ar often seen as the gold standard for making causal inferences about impacts and are more widely used than in any other field of study there is strong reluctance to rely on single rct study according to the cochrane collaboration an intern ational nonprofit gr oup founded in 1993 that supports the production and dissemination of rct information about health care interventions most single rcts are seen as not sufficiently robust against the effects of chance and often ving limited external validity39 moreover there have been widespread concerns about the quality of individual studies of any design the cochrane collaboration has said that the amount of information about health care including from individual rcts is ove rwhelming but that much of what information is availabl is of poor quality in seeking to address questions of how to use single studies and how to judge study quality two major strategies that researchers have used include 1 classifying study types within hierarchies of evid ence and 2 conducting systematic reviews study quality hierarchy of evidence in recent years there has been considerable debate on how to define what quality s hould mean when describing evaluations some observers in medicine and the social sciences hold the view that rcts should be placed at the top of hierar chy in view of an rcts potential for high internal validity in estimating an impact due to concerns about the varying quality of individual studies however some participants in health care evaluation have reoriented their approach40 t h u s p r v n t i v s r v i c s t s k f r c uspstf an independent panel of experts in primary care and prevention that systematically reviews evidence regarding clinical preventive services offered this assessment for some years the standard approach to evaluating the quality of individual studies was based on hierarchical gradin g system of research design in which rcts received the highest score th maturation of critical appraisal techniques has drawn attention to the limitations of this approach which gives inadequate consideration to how well the study was conducted dimensionhttpwikileaksorgwikicrsrl33301crs18 41 agency for healthcare research and quality us preventive services task force current methods of the us preventive servic task force review of the process p m21 2001 available at http wwwahrqgovc linicajpmsupplreviewpdf hereafter uspstf report see also earl p steinberg and bryan r luce evidence based caveat emptor health affairs janfeb 2005 pp 8284 cohort st udy has been defined as an observational study in which outcomes in gr oup of patients that received an intervention are compared with outcomes in similar gr oup ie the cohort either contemporary or historical of patients that did not receive the intervention see httpwwwnlmnihgovnic hsrhta101ta101014html 42 uspstf report p m21 known as internal validity welldesigned cohort study may be more compelling than an inadequately powered or poorly conducted rct41 in response to these conclusions the uspstf listed hierarchy of research designs but as only one among several inputs to ev aluating the quality of individual studies that hierarchy placed designs in the following rankings largely on the basis of designs potential internal validity rcts controlled trials without randomization also called quasiexperiments with trea tment group and control group whose subjects were not assigned randomly cohort or casecontrol analytic studies observational studies in which similar groups serve as c ontrol and treatment groups multiple time series with or wit hout the intervention uncontrolled experiments which look at the eff ects of an intervention on units over significant amount of time and opinions of respected authorities based on clinical experiencedescriptive studies and case re ports or reports of expert committees 42 to determine studys overall quality however the task force also included realized internal validity including design and implementation aspect s and in addition external validity which the task force considered on par in importance with internal validity thus an rct might rank high in terms of potential internal validity but it might have experienced implementation problems leading to poorly realized internal validity or it might have limited external validity in either case or to provide assurance against chance researchers and decisi on makers often wish to consider other studies including studies other than rcts to inform their thinking and potential decision making two additional uspstf criteria for judging quality included assessments of intern and external validity of all relevant studies for given research question and also the extent to which relevant studies or groups of studies linked interventions directly or indirectly to outcomes of interest these latter efforts relate closely to systematic review systematic review in health care in response to concerns about reliance on single studies and study quality the health care field has broadly embraced systematic review as method for identifying gaps in knowledge drawing whateverhttpwikileaksorgwikicrsrl33301crs19 43 national institutes of health national libr ary of medicine glossary available at httpwwwnlmnihgovnichsrhta101ta101014html 44 metaanalyses also typically incorporate so kind of qualitative decision concerning the validity of the rcts being studied this some times causes situations in which different metaanalyses of the same rcts come to opposing conclusions about an intervention for example see peter jüni et the hazards of scoring the quality of clinical trials for metaanalysis jama vol 282 sept 15 1999 pp 10541060 45 the ways in which evidence is defined identified compiled scrutinized and aggregated will often affect conclusions while there is universally embraced best process for evidence review there is considerable interest in two processes risk analysis which is increasingly applied to the regulation of environmental hazards and systematic review which is increasingly applied in the health care field 46 p alderson s green and jpt higgins eds cochrane reviewers handbook 423 section 92 updated may 2005 available at httpwwwcochranedkcochrane handbookhbookhtmconclusions are possible about interventi ons based on available evidence including impact analyses and thereby helping to inform decision making about research priorities and provision of health care to patients systematic review has been defined as form of structured literature review that addresses question that is formulated to be answered by analysis of evidence and involves objective means of searching the literature applying predetermined inclusion and exclusion criteria to this literature critically appraising the rele vant literature and extraction and synthesis of data from evidence base to formulate findings43 this leads some researchers to place systematic review as an additi onal category above rcts at the top of an evidence hierarchy focused solely on potenti internal validity some researchers also place metaanalysis of rcts or perhaps other intervention studies at the top of an evidence hierarchy metaanalysis is type of systematic review that uses statistical methods to derive quantitative results from the analysis of multiple sources of quantitative evidence44 however for various reasons both c onducting and interpreting systematic review can be challenging and can require caution systematic review like an rct or other evaluation might not be compre hensive for all stakeholders or even necessarily for single stakeholder in assessing their evaluation needs45 systematic reviews typically focus on specific ques tion by looking at specific outcome of interest as described in rele vant studies or chosen by an evaluator eg in health care outcomes like mortality quality of life clinical events however they would not necessarily focus on all important outcomes of interest this might raise issues in the context of evaluating public policies because programs mission might encompass variety of potential outcomes of interest or be difficult to represent with one or more outcome measures furthermor different stakehol ders might not agree about the relative importance of varying outco measures or might be interested in different ones finally alt hough systematic reviews typi cally focus much attention on concerns about internal validity of various studies judgments about external validity or generalizability of findings are often left to readers to assess based on their implicit or explicit decision how applicable the systematic reviews evidence is to their particular circumstances46 unfortunately these judgments are oftenhttpwikileaksorgwikicrsrl33301crs20 47 for discussion of multiple types of systematic reviews and synthesis in program evaluation see david s cordray and robert l fischer synthesizing evaluation findings in joseph s wholey harry p hatry and kathryn newcomer eds handbook of practical program evaluation pp 198231 for discussion that primarily emphasizes systematic reviews of rcts from the second edition of the previous book see robert f boruch and anthony petrosino metaanal ysis systematic reviews and research syntheses in joseph s wholey harry p hatry and kathryn newcomer eds handbook of practical program evaluation 2nd ed pp 176203 gao produced guide to evaluation synthesis defined as syst ematic procedure for organizing findings from several disparate evaluation studies see us general accounting office the evaluation synthesis gaopemd1012 mar 1992 p 6 48 try it and see the economist mar 2 2002 pp 7374 49 robert boruch haluk soydan and dorothy moya the campbell collaboration brief treatment and crisis intervention vol 4 3 autumn 2004 p 227 50 campbell collaboration campbell systematic reviews guidelines for the preparation of review protocols ver 10 jan 1 2001 pp 4 6 available at continuedhindered because many studies provide little information that might assist in assessing external validity systematic review in social sciencerelated areas usage of systematic review in health care raises the question of how systematic review might be used in other contexts eg when evaluating government programs of various types which are often assesse d with social science research methods in program evaluation systematic revi ews have been performed under various names eg evaluation synthesis integrative review re search synthesis and in different ways47 however they have been much less common in social sciencerelated areas than in health care this might be the case in part because rcts and other nonrct evaluations have been relatively more s carce in policy areas related to the social sciences as compared to medicine for example over 250000 rct studies had reportedly been published in the medical literature as of 2002 but about 11000 were known in all of the social sciences combined48 other possible hist orical reasons for relative lack of systematic review in the social sciences compared with health care might include the following comparatively less funding devoted to evaluation more technically challenging research settings and problems eg absence of laboratory controls that can make experimental evaluations more difficult to successfully design and implement increasing the risk that studies might result in evaluation funding being wasted resistance to using rcts disagreements about appropriate ways to evaluate programs and less interest from policy makers and institutions in response to such comparisons some fforts have been undertaken to increase production of systematic reviews in social sciencerelated areas for example group of social science researchers creat ed the campbell colla boration nonprofit organization that promotes the use of systematic reviews in the social sciences in defining evidence the campbell collabor ation has focused pr imarily on rcts and secondarily on quasie xperiments in order to determine impacts49 however the organizations guidelines also allow implementation studies and qualitative research to be included in systematic review50 httpwikileaksorgwikicrsrl33301crs21 50 continued httpwwwcampbellcollaborationorgfraguidelineshtml 51 for related discussion however see crs report rl33246 reading first implementation issu and controversies by gail mccallion and crs report rl32663 the bush administrations program assessment rating tool part by clinton t brass 52 the priority took effect on feb 24 2005 see us department of education scientifically based evaluation methods 70 federal register 3586 jan 25 2005 53 for example see yudhijit bhattacharjee can randomized trials answer the question of what works science vol 307 mar 25 2005 pp 18611863 54 for an overview of nclb see crs report rl31284 k12 education hi ghlights of the child left behind act of 2001 pl 107110 coordinated by wayne riddle for an analysis of this and related legislative hi story see margaret eise nhart and lisa towne contestation and change in national policy on scientifically based education research educational researcher vol 32 oct 2003 pp 3138 recent attention to using rcts in program evaluation the following two subsections briefly illustrate how rcts have been subject of attention in two contexts 1 setting program evaluation policy in one specific policy area education and 2 the citation and use of i ndividual studies or claimed lack thereof to justify policy and budget proposals to congress in this case as component of the george w bush ad ministrations progra m assessment rating tool because this reports purpose is limited to providing an overview of rcts and related issues these cases are not analyzed in detail in the report51 however many of the issues identified in this repor t could be applied to these and other cases controversy in education policy priority for rcts in january 2005 the us department of education ed published notice of final priority in the federal register the notice established departmentwide priority for the use of specific types of program evaluation and especially rcts when evaluating certain education programs52 under the priority ed asserted that rcts were best for determining project effectiveness and with some exceptions would be preferred for funding compared to other evaluation methods if ed determined an rct to be infeasible quasiexperimental design would receive priority over other designs the ed priority has provoked controversy in the education policy area and ev aluation field generally53 authority cited for the ed priority nclb and scientifically based research the ed priority was established at the discretion of the secretary of education and was not required by law howe ver the priority appeared in broader context of program evalua tionrelated statutory provi sions enacted by congress specifically ed cited the elementary and secondary education act of 1965 esea as reauthorized by the child left behind act of 2001 nclb 115 stat 1425 pl 107110 as the statutory authority for establishing the priority54 in its federal register notice ed assertedhttpwikileaksorgwikicrsrl33301crs22 55 us department of education scien tifically based evaluation methods 70 federal register 3586 italics in original 56 115 stat 1425 at 1964 s ection 910137 of nclb 20 usc 7801 many and perhaps most of the references to scientifi cally based research in nclb refer to the research upon which instructional techniques that grantee states and local education agencies use in federally funded programs are to be based 57 the provision says the term includes rese arch that employs systematic empirical methods that draw on observation or xperiment section 9101 37bi of nclb 58 the cited section should have been sec tion 910137 of nclb us department of education scientifically ba sed evaluation methods p 3586 59 for more on ies see us department of education institute of education sciences biennial report to congress 2005 available at http wwwedg ovaboutreports annualiesbiennialrpt05pdfthe esea as reauthorized by the nclb uses the term scientifically based research more than 100 times in the context of evaluating programs to determine what works in education or ensuring th at federal funds are used to support activities and services that work this fi nal priority is intended to ensure that appropriate federally funded projects are evaluated using scientifically based research55 under esea as reauthorized by nclb scientifically based research is defined as research that involves the application of rigorous systematic and objective procedures to obtain reliabl and valid knowledge relevant to education activities and programs56 the statutory definition also enumer ates several kinds of research that are included within the term the first enumerated item explicitly includes research that employs either observatio nal or experimental methods57 in the fourth enumerated item the definition also includes research that is evaluated using experimental or quasiexp erimental designs among experimental and quasi experimental designs the definition ex presses preference for randomassignment experiments section 910137biv thus the statutory definition of scientifically based research does not appear to give high er priority to experimental designs above designs that draw on observation except when contrasting experimental and quasiexperime ntal designs versus one another apparently in light of these definitions eds notice of priority also said the definition of scientifically based research in section 920137 sic of nclb includes other research designs in addition to the random assignment and quasiexperimental designs that are the s ubject of this priority however the secretary considers random assignment nd quasiexperimental designs to be the most rigorous methods to address th question of project effectiveness58 additional statutory provisi ons related to program evaluation in education located within the education sciences reform act of 2002 esra 116 stat 1940 pl 107279 were enacted after nclb nd year before the ed priority was proposed the ed priority did not cite esra but esras provisions privilege rcts in some ways that appear to be re lated to eds subsequent actions esra established eds institute of education sciences ies and set forth its functions59 esras definition for scientifically based research standards holds that when ieshttpwikileaksorgwikicrsrl33301crs23 60 see 116 stat 1943 section 10218 61 see 116 stat 19431944 section 10219 and 116 stat 1962 and 19641965 sections 171b2 172 and 173 62 for the original notice of proposed prio rity see us department of education scientifically based evaluation methods 68 federal register 62445 nov 4 2003 for contrasting views on the subject see 1 t homas d cook randomized experiments in educational policy research critical examination of the reasons the educational evaluation community has offered for not doing them educational evaluation and policy analysis vol 24 3 fall 2002 pp 175199 written before the ed priority but advocating increased use of rcts in educati on policy in spite of objections 2 stewart i donaldson and christina christie t he 2004 claremont debate lipsey vs scriven determining causality in program evaluation and applied research should experimental evidence be the gold standard pp 48 summary and video of debate about the priority between two wellknown evaluation experts available at httpwwwcguedu includesbos_2004_debatepdf online video of the discussion along with selected transcriptions available at httpwwwcg uedupages2668asp and 3 michael quinn patton the debate about randomized cont rols in evaluation the gold standard question lecture delivered at national institute s of health national cancer institute sept 14 2004 cr iticizing the priority and advocating diffe rent approach online video available at httpvideocastnihgovramnci091404ram presentation slides available at httpvideocastnihgov pptnci_patton091404ppt 63 see for example the archived email discussion list listserv of the american evaluation association evaltalk entrie s from nov 4 2003 to jan 4 2004 available at httpbamauaeduarchivesevaltalkhtmlfunded research is intended to make cl aims of causal relationships the ies research should include only random assign ment experiments and other designs to the extent such designs substantially eliminate plausible competing explanations for the obtained results60 esra does not specify what these other designs are or what it means to make claims of causal relationships therefore it appears that claim of causal relationship need not be restricted to evaluations that seek to estimate an impact esras definition for scientifically valid education evaluation holds that when iess national center for education evaluation and regional assistance conducts evaluations to estima the impact of programs it should employ experimental designs using random assignment when feasible and other research methodologies that allow for the strongest possible causal inferences when random assignment is not feasible61 these esra definitions hold for funding controlled by ies not for th entire education department reactions to the priority originally proposed in november 2003 the ed priority generated considerable debate in the evaluation field62 the priority did not explicitly define the word effectiveness as noted earlier in this report effectiveness is program evaluation term that has been used in multiple ways ie synonym for impact goal achievement or merit and wort h upon close reading the prioritys usage of the term appeared to indicate the term was probably used in most cases as synonym for impact however the prioritys use of the term effectiveness and the phrase what works appeared to be interpreted by many observers to go beyond the definition of impact 63 in addition some text in the priority appeared to behttpwikileaksorgwikicrsrl33301crs24 64 of the nearly 300 commenters ed said that 29 expressed support for the priority ed did not tabulate how many commenters opposed the prio rity however in eds analysis of the comments several of eds categories of comme nts appeared to reflect criticisms of the priority and the number of commenters in some of those areas ranged from 168 to 242 65 see us department of education strategic plan 20022007 mar 2002 p 53 available at httpwwwedgovaboutreportsstratpla n200207indexhtml this document pre dated proposal of the ed priority by over yearinterpreted to claim rcts we re best for demonstrating causation even apart from estimating an impact during the proposed prioritys month long comment period nearly 300 parties sent comments to ed these comments were summarized and analyzed in the ed january 25 2005 notice of final priority wh ich also included statements from then secretary of education rod paige regarding where he agreed or disagreed with comments that were submitted from eds summary and categorization of the comments it appears many more comments were critical of the priority than supportive64 ed determined that while the comments it received were substantive the comments did not warrant changes in the priority both prior to and after publication of the ed priority many observers who supported the priority as well as some who opposed the ed priority agreed that more rcts were needed in education policy in certain circumstances in order to estimate impacts of different educational interventions many also agreed that rcts had been unjustifiably deemphasized in the past compared to other evaluation methods to greater or lesser extents furthermore many supporters of the ed priority argued the priority was an appropriate change in the education field because they believed more information about the impacts of educational interventions was needed to help inform practitioners and policy makers and b ecause they believed eds research agenda had been previously influenced by hostility to rcts and similar types of studies however many critics of the ed priority argued that rcts have been oversold in terms of their practical capabilities that the priority unjustifiably deemphasized other evaluation designs in terms of their practical capabilities to contribute to understanding of causes effects impacts effectiveness and in some cases to making claims of causal relationships even if some of the designs are not intended to cal culate impacts and that the ed priority would detrimentally affect ove rall priorities for evaluation implications and re lated developments to the extent that evaluations help frame future choices it appears the way in which the ed priority will be implemented could affect the future course of education programs and policy at minimum the priority has influenced th use of hundreds of millions of research dollars controlled by ed and arguably education policy as implemented by ed for example eds departmentwide strategic planning documents and activities appear to reflect the priority the ed strategic plan established goal that 75 of new research and evaluation projects funded by the department that address causal questions employ randomized experimental designs65 the question of what types of research can make causal claims or make causal claims while substantially eliminating plausible competing explanations has often been contentious in thehttpwikileaksorgwikicrsrl33301crs25 66 many arguments about this subject concer n definitions of causation and explore what types of knowledge different methods of re search are capable of discovering some observers give rcts privileged place among ev aluation methods in making causal claims arguing it is theoretically the best way to avoi d threats to internal validity others dispute that contention arguing that rcts are not always appropriate many methods can make scientifically valid causal claims eg citin g the effort to prove the causal relationship between smoking and cancer and rcts ofte n depend on the support of other methods to justify internal validity claims sometimes researchers in variety of fields arguably answer this question of what methods are cap able of making causal claims according to their training and preferred research techni ques one scholar called this phenomenon when it occurs the law of the instrument under which some preferred set of techniques will come to be identified with scientific method as such see abraham kaplan the conduct of inquiry methodology for behavioral science scranton pa chandler 1964 pp 2830 67 us department of education fiscal year 2007 performance budget feb 2006 p 3 available at httpwwwedgovaboutreportsannual2007planfy07perfplanpdf 68 us department of education strategic plan 20022007 p 80 69 see us department of education institute of education sciences national center for education evaluation and regional assistance prepared by the coalition for evidence based policy identifying and implementing educati onal practices supported by rigorous evidence user friendly guide washington us departme nt of education dec 2003 p iii available at http wwwedgovrschs tatresearchpubsrigorousevidindexhtml 70 ibid pp iii v 11 and 17evaluation field and the philosophy of science66 the ed performance budget accompanying the departments fy 2007 budget pr oposal contains departmentwide objective to encourage the use of scien tifically based met hods within federal education programs67 one performance measure is listed for that objective the proportion of schooladopted approaches that have strong evidence of effectiveness compared to programs and interv entions without such evidence the ed strategic plan also states that the department will seek funding for programs that work and will seek to reform or eliminate programs that do not68 the strategic plan does not define what is meant by the word work but the word might mean increases student achieveme nt in some cases however if ed determined that some prog rams increase student achie vement but at undesirable cost or with unintended side effects then presumably ed would not seek funding for those programs thus the word work might instead be intended to indicate merit and worth in some cases month after the priority was origin ally proposed ed issued prominent guidance document to provide educational practitioners with userfriendly tools to distinguish practices supported by rigo rous evidence from those that are not69 the ed guidance asserted that evaluation thods other than rcts and certain quasi experiments 1 have meaningful evidence to contribute to establishing whether an intervention was effective and 2 cannot be considered scientificallyrigorous evidence or rigorous evidence to support using an educational practice to improve educati onal and life outcomes for children70 the document appears to define evidence that would support these decisions to include only estimations of impact the document also cites nclb as calling on educational practitioners to usehttpwikileaksorgwikicrsrl33301crs26 71 see httpwwwwhatworksedgovreviewprocessstandardshtml 72 see httpwwwwhatworksedgovfaqwhat_is_wwchtml 73 see reports listed at httpwwwwhatworksedgovproducts browsebylatestreportsresultsaspe videncerptid03reporttypeall 74 see httpwwwwhatworksedgovreviewprocessnotmeetscreenshtml 75 see httpwwwwhatworksedgovreviewprocessstandardshtml 76 see httpwwwwhatworksedgovreviewprocessnotmeetscreenshtml 77 us government accountability office performance measurement and evaluation definitions and relationships gao05739sp 78 strictly speaking the phrase what works ght be at odds with what the wwc presents instead the phrase what may have worked might be more appropriate for what the wwc presents in common speech the phrase what wo rks appears to assert that if program previously worked in one instance the pr ogram can be expected to work in other instances that is that the programs results are generalizable to other contexts times or places however the wwc website is careful to say in its study reports single study report should be used as basis for making policy decisions because 1 few studies are designed and implemented flawlessly and 2 all studies are tested on limited number of continuedscientificallybased research to guide th eir decisions about which interventions to implement but does not disc uss roles many observers argue that other evaluation methods can play in complementing bolst ering or undermining an rcts findings in addition the departme nt established what works clearinghouse wwc to evaluate the strength of the evidence of effectiveness of educational interventions to help educators and education policymakers incorporate scientifically based research into their educational decisions71 in this formulation it appears the wwc might be using the word effectiveness and the phrase what works as synonyms for showing favorable impact on an outcome of interest according to the wwc website the wwc was established in 2002 by the us department of educations institute of education sciences ies to pr ovide educators policymakers researchers and the public with central and trusted s ource of scientific evidence of what works in education72 wwc study reports state that neither the what works clearinghouse wwc nor the us depa rtment of education endorses any interventions73 the wwc states it includes only certain evaluation designs in its databases which happen to be only the designs that were listed in the ed priority74 because they provide the strongest evidence of effects75 it appears the term effects is used here as synonym for impacts the wwc excludes other types of evaluations because the wwc asserts they are not outcome evaluations76 however in the latest government accountability office gao pamphlet on definitions of major types of program ev aluation gao defined outcome evaluation to also include evaluations that focus on unintended eff ects and that assess program process to understand how outcomes are produced77 these other types of evaluations would appear to be excluded from the wwc nd perhaps from ed determinations of what works78 in addition the ed strategic plan states under the departmentshttpwikileaksorgwikicrsrl33301crs27 78 continued participants using limited number of outcomes at limited number of times so generalizing from one study to any contex t is very difficult see for example httpwwwwhatworksedgovpdfridgway_2002_brief_study_reportpdf 79 us department of education strategic plan 20022007 p 16 80 see httpwwwwhatworksedgovfaqwhat_researchhtml the website attributed the definition to the ies 81 see httpwwwwhatworksedgovfaqwwc_nclb_readfirsthtmlobjective 14 that every ed program will develop what works guide to be distributed to program grantees that whe never possible will be informed by the what works clearinghouse79 therefore it appears that ed and ies determinations of what works might be drawn primarily or only fro m studies that include rcts certain quasiexperiments and the exceptions allowed for in the ed priority and not other kinds of program evaluation at the time of this reports writing th wwc website presents definition for the term scientifically based research that is at variance with the terms definition under nclb80 as noted previously the nclb definition for scientifically based research includes observational studies and ex perimental studies expressing preference for rcts over quasiexperiments but not expressing preference for rcts over other evaluation research designs even when questions of causation are being examined however the wwc website presents different definition for the term scientifically based research specifically the wwc website instead presents what appears to be the esra definition for scientifically based research standards which applies only to iesfunded resear ch the esra terms definition for scientifically based research standards holds as noted earlier that when iesfunded research is intended to make claims of causal relationships the ies research should include only random assignment ex periments and other designs to the extent such designs substantially eliminate plausible competing explanations for the obtained results although esra does not indicate what thes other designs could be the wwc appears to include only the designs mentioned in the ed priority in that category nevertheless the ed priority was established for the entire department at the same time the wwc website appears to tie wwc to nclbs term scientifically based research 81 in effect as demonstrated through ed policy and strategic planning documents the ed priority appears in some respects to be extending eds interpretation of esra definitions and preferences for certain types of evaluations especially rcts beyond ies to the entire education depa rtment notwithstanding the apparently differing definitions and pref erences expressed in nclb assessing programs in the budget process the part the bush administrations program assessment rating tool part in 2004 the bush administration evated rcts as the preferred way to evaluate federal executive branch pro grams under portions of its programhttpwikileaksorgwikicrsrl33301crs28 82 for an overview and analysis of the part see crs report rl32663 the bush administrations program assessment rating tool part by clinton t brass the definitions that omb used for the term program when conducting part assessments have been criticized by some observers ombs definitions of programs to be assessed which were typically determined by budgetary perspective sometimes aggregated several activities into single part program or di saggregated group of activities into several part programs 83 the administrations usage appears to go beyond typical program evaluation definition of effectiveness which refers to achievement of goal because under the part omb makes judgments about programs mission and performance goals eg whether they address specific and existing problem intere st or need whether they are unclear because of multiple and overlapping objectives and whether they are effectively targeted meaningful and ambitious and programs costeffectiveness the administration has also for example referred to the part as proving programs worth us office of management and budget budget of the united stat government fiscal year 2004 p 51 nevertheless the administration appears to distinguish between the term overall effectiveness and its views about priorities for budget proposal eg according to its fy2006 listing these priorities were defense homeland security economic opportunity and fostering compassion nd its views about which programs missions have an appropriate federal role see us office of management and budget major savings and reforms in the presidents 2006 budget washington gpo 2005 p 4 assessment rating tool part initiative82 the part is set of questionnaires that the office of management and b udget omb develope d in 2002 and annually revised thereafter to determine the overall effectiveness of programs included in the presidents annual budget proposal lthough omb did not provide an explicit definition for the term overall effectiveness omb and the bush administration appeared to use the term as at least partial synonym for merit and worth 83 the administration has used the part with some controversy to justify budget proposals and the proposed elimination or reduction of many programs omb first presented the part to congress for the fy2004 budget cycle assessing programs that re presented approximate ly 20 of the federal budget for succeeding budget cycles omb said that cumulative 20 increments of federal programs would be assessed with the part in addition to some reassessments of programs previously parted the ad ministration subsequently released part ratings for selected programs along with the presidents fy2004 fy2005 fy2006 and fy2007 budget proposals an additiona l round of ratings is planned to be released with the presidents fy2008 budget proposal with the final year assessing all remaining executive branch spending and programs thereafter all programs would presumably be assesse d or reassessed each year depending on how part questionnaire is filled in and evaluated for program it will produce single numerical score in percentage terms between 0 and 100 this figure termines the programs overall effectiveness rating four ratings are possible based on the score effective score of 85 to 100 moderately effective 70 to 84 ad equate 50 to 69 and ineffective 0 to 49 omb characterizes these ratings as qualitative rather than quantitative different designation was created regard less of part score for programs that omb decided do not have acceptable performance measures or have not yethttpwikileaksorgwikicrsrl33301crs29 84 us office of management and budget budget of the united stat government fiscal year 2005 analytical perspectives washington gpo 2004 p 10 85 see us general accounting office performance budgeting ob servations on the use of ombs program assessment rating t ool for the fiscal year 2004 budget p 25 86 ibid 87 us office of management and budget budget of the united states government fiscal year 2005 analytical perspectives washington gpo 2004 p 9 88 us office of management and budget budget of the united states government fiscal year 2004 analytical perspectives washington gpo 2003 p 9 89 us office of management and budget performance measurement challenges and strategies june 18 2003 p 4 availabl at http wwwwhitehousego vombpart challenges_strategiespdf 90 some but not all of these programs had been assessed with the part see us office of management and budget major savings and reforms in the presidents 2006 budget 91 amelia gruber the big squeeze government executive feb 2005 p 48 92 of the 48 ed explicitly cited 7 in the departments summary budget justification as receiving the part rating results not monstrated ie unknown results and 3 as receiving the part rating ineffective see us department of education fiscal year 2006 budget summary and background information feb 7 2005 pp 7279 available at httpwwwedgovaboutoverviewbudgetbudget06summary06summarypdf or at http wwwedgovaboutove rviewbudgetbudget06summaryedlitesection3htmlcollected performance data84 the designation was called results not demonstrated gao has said it is important for users of the part information to interpret the results not demonstrat ed designation as unknown effectiveness rather than as meaning the program is ineffective85 gao also found that disagreements between omb and agencies on appropriate performance measures for certain programs helped lead to the results not demonstrated designation being given to these programs for purposes of the part86 use of the part while the administration has called the part management tool87 it has also said the parts overall purpose is to lay the groundwork for funding decisions88 furthermore the administration also provided guidance to agencies that program s part questionnaire should include good performance goals that provi information that helps make budget decisions but need not include performance goals to imp rove the management of the program89 in the context of the presidents fy2006 budget proposal which called for terminating or substa ntially reducing 154 disc retionary programs90 an omb official reportedly said we have to focus more resources on what works and the part is the primary tool to make that judgment91 of 99 discretionary programs proposed by the administration for termination for fy2006 48 or nearly half of all proposed terminations were in the department of education92 in the administrations justification document rct evaluationshttpwikileaksorgwikicrsrl33301crs30 93 us office of management and budget major savings and reforms in the presidents 2006 budget p 25 for analysis of the ju stification see crs report rl33071 even start funding controversy by gail mccallion 94 see us office of management and budget major savings in the 2006 budget washington dec 22 2005 available at http wwwcqcombudgettracker do newsletter for jan 9 2006 and at httphotlineblognationaljournalcomarchives 2005 12indexhtml listed under web log entry entitled wh touts budget successes see also white house office of communications fis cal year 2006 keeping the commitment to restrain spending press release dec 22 2005 available at httpwwwwhitehousegovnewsreleases2005122005122215html 95 us office of management and budget fiscal year 2007 budget of the us government washington gpo 2005 p 2 96 ibid p 83 97 see us department of education fis cal year 2007 budget re quest advances nclb implementation and pinpoints competitiveness presidents budget supports new math and science instruction and high school refo rm targets resources and eliminates 42 programs proven ineffective saving 35 billion press release feb 6 2006 available at httpwwwedgovnewspressreleases20060202062006html and us department of education fiscal year 2007 budget summary and background information feb 6 2006 available at httpwwwedgovaboutoverviewbudgetbudget07summaryindexhtml 98 for example see us department of education fiscal year 2007 justifications of appropriation estimates to the congress volume ii washington feb 2006 p m30 99 us office of management and budget what constitutes strong evidence of programs effectiveness undated white paper 2004 p 1 hereafter cited as the omb continuedwere explicitly cited as supporting termination of th ed even start program93 and the administration further cited lack of pe rformance information or lack of rigorous evaluations to support termination of many of the other ed programs on december 23 2005 the white house reportedly sent to reporte rs and surrogates listing of terminations accepted in whole or in part by congress which some media posted on their websites94 neither the white house nor omb posted the document on their websites according to the omb document 17 of the 49 proposed ed terminations were accepted by congress in whole or in part including 56 cut in even start for the presidents fy2007 budget proposal the administration proposed 141 programs that should be terminated or significantly reduced in size governmentwide95 of these the administration proposed to terminate 42 programs within the depar tment of education nearly 30 of the governmentwide total proposed for ter mination or major re duction including many that the part has shown to be ine ffective or unable to demonstrate results96 the department said in press release th 42 programs were proven ineffective97 ed also stated in budget justification do cuments that request ed termination was consistent with the depar tments goal to eliminate support for programs that show limited or evidence of effectiveness98 rcts and the part for purposes of the part in 2004 omb elevated rcts as the preferred way to evaluate pr ograms effectiveness with release of document entitled what constitutes strong evidence of programs effectiveness99httpwikileaksorgwikicrsrl33301crs31 99 continued what constitutes document available at httpwwwwhitehousegovombpart2004_program_evalpdf 100 cebps mission is to promote government policymaking based on rigorous evidence of program effectiveness cebp has particular ly emphasized the advantages of rcts for that purpose and the disadvantages of using other ev aluation research designs for more on the groups purpose and agenda see http wwwex celgovorgindexphpkeyword a432fbc34d71c7 101 see httpcoexgovsecuresitesneti ndexphpkeyworda432fbc34d71c7 for cebps description of its role in working with omb see also amelia gruber the big squeeze government executive p 53 for the most recent iteration of ombs guidance for the part see us office of management and budget guidance for completing the program assessment rating tool part mar 2005 available at h ttp wwwwhitehousegov ombpartfy20052005_guidancedoc the guidance was intended to be used for the presidents fy2007 budget proposal but appeared to be labeled fy2005 to indicate the fiscal year in which it was released ombs part guidance from previous years is available in electronic form or hard c opy from this reports firstlisted author 102 omb what constitutes document p 1 in cases when it is not possible to use rcts to evaluate program impact the document di rects agencies to consult with internal or external program evaluation experts as ppropriate and omb to identify other suitable evaluation methodologies to demonstrate prog rams impact but provides little explicit guidance in that regard ibid p 3 this document intended to provide guidan ce to agencies on appropriate evaluations particularly highlighted rcts as best for evaluating effectiveness the document was apparently written with the assistance of the coalition for evidence based policy cebp an organization s ponsored by the council for excellence in government100 according to cebps webs ite the part and ombs annual guidance to agencies for the part i n this case for the fy2006 budget were revised as result of collaboration between ce bp and omb to endorse randomized controlled trials as the pr eferred method for measuring program effectiveness and wellmatched quasiexper imental studies as possible alternative when randomized trials are not feasible101 nevertheless the omb document also stated that rcts are not suitable for every program and generally can be employed under very specific circumstances therefore agencies often will need to consider alternative evaluation methodologies in addition even where it is not possible to demonstrate impact use of evaluation to assist in the management of programs is extremely important102 in ombs what constitutes document omb used the term effectiveness in at least two senses 1 arguably as partial synonym for merit or worth consistent with the parts usage or 2 referring to demonstrating impact which omb defined as the outcome of program which otherw ise would not have occurred without the program intervention these concepts do not necessarily repres ent the same thing because merit or worth can be judged by many other factors in addition to impact onhttpwikileaksorgwikicrsrl33301crs32 103 for example in the view of stakeholders additional determinants of merit or worth could in addition to impact on particular out come of interest include timeliness quality cost efficiency replicability of program implemen tation or results at other times or in other contexts presence of unintended outcomes im pact on outcomes of interest that may not have been chosen by omb or an agency to assess the program achievement of missionrelated goals that are not necessarily encompassed by quantitative measures of impact program or environmental changes that make past estimations of impact obsolete the comparative merit and worth of policy alternatives and values eg stakeholders normative views on the programs importance goals or means of implementation 104 this gold standard claim has been contentious in some fields including medicine and especially in education in ombs footnot supporting the gold standard assertion omb included literature supporting the assertion eg an article from the journal of economic perspectives calling for increased use of rcts but omitted or ignored literature arguing against the assertion eg the adjacent article from the same issue of the journal of economic perspectives which argued the case for rcts was overstated compared to alternative designs except for some aspects of internal validity see respectively gary burtless the case for randomized field trials in economic and policy research journal of economic perspectives spring 1995 pp 6384 and james j heckman and jeffrey smith assessing the case for social experiments journal of economic perspectives spring 1995 pp 85110a specific outcome of interest103 in some cases the what constitutes document is clear which definition of effectiveness is being used eg when using the term impact however other instances are less cl ear or could potentia lly be interpreted by agencies as treating the definitions impact and merit and worth equivalently sampling is reprinted below the part was developed to assess the effectiveness of federal programs and help inform management actions budget requests and legislative proposals directed at achieving results p 1 the revised part guidance this year underscores the need for agencies to think about the most appropriate ty pe of evaluation to demonstrate the effectiveness of their programs as such the guidance points to the rct as an example of the best type of evaluation to demonstrate actual program impact p 1 few evaluation methods can be used to measure programseffectiveness where effectiveness is understood to mean the impact of the program p 1 the most significant aspect of program effectiveness is impact the outcome of the program which otherwise would not have occurred without the program intervention p 2 italics in original nonexperimental direct analysis studies often lack rigor and may lead to false conclusions if used to measure program effectiveness and therefore should be used in limited situations and only when necessary such methods may have use for examining how or why program is effective or for providing information that is useful for program management p 3 italics in original welldesigned and implemented rcts are considered the gold standardfor evaluating an interventions effectiveness across many diverse fields of human inquiry such as medi cine welfare and employment psychology and education p 4 104httpwikileaksorgwikicrsrl33301crs33 105 us office of management and budget i nstructions for the program assessment rating tool mar 22 2004 pp 24 47 available at http wwwwhite housegovombpart 2006_part_guidancepdf 106 us office of management and budget guidance for completing the program assessment rating tool part mar 2005 p 28 107 us government accountability office program evaluation ombs part reviews increased agencies attention to im proving evidence of program results gao0667 oct 2005 pp 2225 and performance budgeting part focuses attention on program performance but more can be done to engage congress gao0628 oct 2005 pp 26 27ombs annual guidance to agencies for the fy2006 budgets part similarly elevated rcts as the preferred way to assess effectiveness and impact the guidance called impact the most significant aspect of program effectiveness called rcts generally the highest quality unbiased ev aluation to demonstrate the actual impact of the program and further asserted that the most definitive data supporting programs overall effectiven ess would be from an rct when appropriate and feasible105 the guidance also stated that rcts are not suitable or feasible for every program because federal programs vary so dramatically in such situations the guidance suggested welldesigned quasiexperimental studies as another way to assess impact and other types of evaluations to help address how or why program is effective or ineffective italics in original for the fy2007 part however the tenor of ombs statements in its guidance about rcts may have changed to some gree ombs discussion in the revised guidance largely mirrored that of th fy2006 guidance but eliminated the description of rcts as the highest quality unbiased evaluation to demonstrate the actual impact and replaced it with langua ge calling rcts partic ularly well suited to measuring impacts106 when rcts are judged by omb and agencies to not be feasible or suitable the guidance exhorts agencies and omb to consult with inhouse or external evaluation experts and dir ects them to supplemental guidance in the what constitutes document judging success in practice it appears that ombs judgments regarding quality and suitability of evaluation signs have sometimes trumped agency judgments and therefore determined what ev aluation methods are to be used for the part in spite of disagreements between omb and agencies107 disputes about the proper ways to judge program success have also emerged for example in the context of controversy over the administr ations ineffective part rating of the community development block grant cdbg program and fy2006 budget proposal to significantly cut and consolidate 18 community and economic development programs an article quoting ombs deputy director for management clay johnson iii suggested that po litical views or at l east different views about program goals might play role johnson acknowledges that cdbg fails th part test in part because the administration is applying new definiti on of success we believe the goal of housing programs is not just to build houses but the economic development that comes with them so those are the results we want to focus on johnson saidhttpwikileaksorgwikicrsrl33301crs34 108 paul singer by the horns national journal mar 26 2005 p 904 observers scholars and even statutes often have diffe rent definitions of terms like outcome output impact and variety of other program evaluation terms and different views about the importance or applicability of those terms under gpra outcome measure refers to an assessment of the results of program activity compared to its intended purpose and output measure refers to the tabulation calculation or recording of activity or effort and can be expressed in quantitative or qualitative manner 31 usc 1115 109 us office of management and budget major savings and reforms in the presidents 2007 budget washington feb 2006 p 27 available at httpwwwwhitehousegovombbudgetfy2007pdfsavingspdf 110 see httpwwwwhitehousegovombexpectmoresummary100002122005html 111 marsha silverberg et us department of education office of the under secretary policy and program studies service national assessment of vo cational education final report to congress june 2004 available at httpwwwedgovrschstat evalsectechnaveindexhtml the report pr esented synthesis of evidence on the implementation and outcomes of vocational edu cation and the 1998 perkins act see p 16 according to the studys authors the nave was conducted on an independent basis as called for by law and did not necessarily reflect official views or policies of ed p xvi for discussion of perkins iii and also the nave study see crs report rl 31747 the carl d perkins vocational and technical education act of 1998 background and implementation by rebecca r skinner and richard n apling 112 us office of management and budget major savings and reforms in the presidents 2007 budget p 27 see also us department of education fiscal year 2007 budget summary and background information p 51you can say we are imposing our political views on people or our favored views of the housing world or the cdbg world on people well guilty as charged its important to focus on outcomes not outputs108 another example might be what omb s called for purposes of the part the vocational education state grants program within ed which the administration proposed for termination for the fy2007 budget109 this program is the largest budgetary component relating to the carl d perkins vocational and technical education act of 1998 co mmonly called perkins iii for the fy 2007 budgets part the administration deemed the program ineffective the lowest part rating110 the administration justified termination by citing particular study the national assessment of vo cational education nave111 stating that the nave found evidence that high school vocati onal courses themselves contribute to academic achievement or college enrollment112 this perspective appears to consider academic achievement and colle ge enrollment to be the go als of federally supported vocational education however the june 2004 nave also found that the short and mediumterm benefits of vocational education are most clear when it comes to its longstanding measure of success earnings citing research findings that students earned almost 2 percent more fo r each extra high school vocational course they took extending to varying degrees t the large group of high school graduates who enroll in postsecondary education and training to both economically andhttpwikileaksorgwikicrsrl33301crs35 113 marsha silverberg et us department of education office of the under secretary policy and program studies service national assessment of vo cational education final report to congress pp xixxx 18 and 266 114 ibid pp xix 265 115 see httpwwwwhitehousegovombbudgetfy2004pmavocationaleducationxls question 45 for the fy2004 budgets part assessment the worksheet is available in microsoft excel format for the fy2007 budget part assessment see httpwwwwhitehousegovombexpectmoredetail100002122005html 116 for example see aimee curl supporters call even start case study in faulty program assessments federal times jan 9 2006 p 4 and us general accounting office performance budgeting observations on the use of ombs program assessment continuededucationally disadvantaged students to those with disabilities and to both men and women113 the study authors further observed perkins iii and its legislative pred ecessors have largely focused on improving the prospects for students who take vocationa l education in high school group that has historically been considered low achieving and noncollege bound however students who participate most intensiv ely in vocational progams are actually quite diverse the vocational courses most high sc hool students take improve their later earnings but have effect on other outcomes that have become central to the mission of secondary education such as improving academic achievement or college transitions whether the program as currently supported by federal legislation is judged successful depends on which outcomes are most important to policymakers114 the part assessment of the program released with the fy2007 budget was originally released with the fy2004 budg et in february 2003 reflecting data available in 2002 and had not been update d to reflect the june 2004 nave the part assessments worksheet provided only brief reference to evidence regarding the impact of vocational education on earnings115 neither the administrations justification document for terminating the program nor eds fy2007 budget summary mentioned earnings benefits of fe derally supported vo cational education disputes about an existing programs pr oper goals can raise questions about the construct validity of studies that purport to evaluate the program regardless of whether the study is an rct the part or another type of evaluation how should one measure success what outcome of interest or outcomes are the most important ones proponents of the part have viewed favorably the initiatives effort to raise program performance to more salient place in budget deliberations many observers have also seen favorably the parts transparency with detailed justifications for the administrations views available on the web for consideration by congress and the public however critics and other observers have said the administrations criteria for evaluating programs sometimes deviated from the programs purposes as determined by congress or that the administration and omb substituted their views about appropriate program goal s and measures over those developed by agencies under the statutor framework established by gpra which explicitly provides for stakeholder views including those of congress116httpwikileaksorgwikicrsrl33301crs36 116 continued rating tool for the fiscal year 2004 budget gao04174 jan 2004 pp 67 117 for example congress might consider legi slation to set program evaluation policy in additional areas or might conduct oversight over program evaluation policies that are already in statute in addition congress migh t consider directing funding interpreting or scrutinizing specific evaluations during its lawmaking and oversight work there is little doubt that participants and stakeholders in the policy process will bring program evaluations to congress to try to influence the thinking and decision making of members and committees 118 for discussion regarding ethical dimensions to random assignment see the section below entitled privacy ethics and study oversightpotential issues for congress the previous section of this report illustrated how rcts have been subjects of prominent attention in two contexts 1 setting program evaluation policy and 2 citation and use of individua l studies or lack thereof to justify policy and budget proposals to congress in these and potentially other cases focus on rcts might raise multiple issues for congress117 some relate specifically to rct studies including an rcts structural requirements and constraints ot her issues relate to program evaluation generally and therefore to rcts number of these issues are identified and analyzed below issues when directing or scrutinizing rcts if congress wants to focus on rcts in th context of program evaluation policy eg developing legislation for or conducting oversight over program agency or the entire government or prospectivel deciding whether to fund specific evaluations congress might consider num ber of issues related to the parameters of these studies and prospective risks to their internal and external validity in addition issues could arise if congress want s to interpret or scrutinize individual rct studies eg when they are presented to congress in the budget or authorization processes considering study parameters when making program evaluation policy congress might opt to focus on some key parameters of studies including random assignment the cost of an rct the length of time that an rct would take before producing findings and privacy or ethical considerations random assignment as discussed earlier the central attribute of an rct is the random assignment of subjects to treatment and contro l groups which helps researcher to make inferences that pa rticular intervention was responsible for an impact and to estimate that impact with reliable statistical tools118 i n s m programs however it may not be feasible or costeffective to randomly assign units to an intervention group and control group for example it is not possible to conduct an rct on whether policy regula ting the release of chlorofluorocarbons into the environment contributes to overall global warming because there is only one planet earth to study thus if congress is considering whether to require certainhttpwikileaksorgwikicrsrl33301crs37 119 for example potential benefits could be reallocation of funds if the program is determined to be failure and not appropriate to be modified or fixed or improved accomplishment of the mission if ways are found to improve the program 120 omb what constitutes document pp 11 18 121 coalition for evidencebased policy bringing evidencedriven progress to crime and substanceabuse policy r ecommended federal strategy p 13 available at httpwwwexcelgovorgusermediaimagesuploadspdfsfinal_report__evidencebas ed_crime__subs_abuse_policypdftypes of evaluation for program or if c ongress is asked by an actor in the policy process to change funding for program due to lack of experimental evidence of program impact it might be important to question whether random assignment is possible or practical on the other hand if program would appear to allow for an evaluation using random assignment but none is planned or an alte rnative is planned eg quasiexperiment it might be important to consider whether an rct evaluation would be more appropriate for example with respect to the cases discussed in this report what programs ssessed by the part are practically suitable for evaluation by an rct under the ed priority why should quasiexperiments also receive priority for funding in addition to rcts cost of rcts if congress considers setting evaluation policy or directing specific studies for an agency or progra m it might take the likely costs of an evaluation method or study into considera tion to weigh against the studys potential benefits119 large scale multisite rcts are typically expensive especially when the units being studied are not individual pe rsons but rather organizations such as schools or jails large multisite rcts of k12 education funded by ed have reportedly cost 10 million to 50 million120 in many cases this level of funding is not available for the study of federal program due in part to tight budgetary constraints and the fact that program eval uations frequently must be paid for out of programs budget smaller scale rcts featuring the random assignment of 100 200 individuals might be relatively in expensive reportedly costing from 300000 to 700000121 even this cost however can often be as much as studied programs funding level quasiexperiments are fre quently but not always less expensive but also might bring different set of potentia l benefits and risks compared to an rct even if considerable funding is availa ble for evaluations pursuing particular evaluation will leave fewer resources for other evaluation needs that might be judged important the opportunity cost of pursuin g certain evaluations rather than others ie cost associated with opportunities that are foregone by not putting resources to their highest value use can be high nevertheless because the potential benefits and costs of evaluations can vary widely depe nding on an agencys or programs portfolio of evaluation needs weighing these consid erations can be difficult in addition priorities might change depending on developments in an agency or policy environment in light of these and othe r considerations congress might weigh the possible benefits of study against the likel cost in view of the broader portfolio of evaluation needs mindful of the risk that study might be poorly designed or implemented or suffer from contamination that reduces confidence in the studys findings evaluation designs that do not offer the theoretical advantages of rctshttpwikileaksorgwikicrsrl33301crs38 122 this section of the report discusses ex isting executivebranchwide statutory and regulatory provisions that might provide certain kinds of protection to study participants it does not examine provisions that might be pr ogram or agency specific for discussion of the rights of human subjects in program evaluations and guidance for the program evaluation field see joint committee on sta ndards for educational program evaluation the program evaluation standards 2nd ed thousand oaks ca sage 1994 see also the website of the office of human subjects research national institutes of health regarding regulations and ethical guidelines ava ilable at httpohsrodnihgovguidelines guidelineshtml 123 the act does not specifically mention progra m evaluations but regulates how executive branch agencies may maintain collect use and disseminate information about individualsregarding internal validity might or might not be worthwhile alternatives to rcts depending on congresss evaluation obj ectives in specific situations length of time to yield findings rcts might take long period of time to yield findings that can inform thinking and policy decisions for example an rct that aims to estimate whether certain aftercare program reduces the recidivism of juvenile offenders must follow the progra ms graduates and the control group over multiyear time span this elongated time window might be pr oblematic if policy decisions need to be made expeditiously however the length of time necessary to conduct an rct and perhaps other comp lementary evaluations might justify waiting to make policy decision until more evidence becomes available furthermore programs or the external environment might have changed by the time the old program was evaluated how s hould the possible benefits of evaluations be weighed against risks of evaluation information becoming dated or obsolete what are the implications for the types and extent of evaluation research to be conducted congress could be called upon to consider these issues if it chose to establish program evaluation policy or di recting and funding specific evaluations privacy ethics and study oversight when considering whether to direct the use of rcts or other evaluations of public programs and policies congress might consider whether the agencies conducting them are charged or should be charged with ethical duties to protect rct study participants privacy access to programs and opportunity to give inform ed consent in addition if congress determined that an agency should be char ged with the ethical duties congress might consider requiring oversight by law regulation or instituti onal action to help ensure that the duties were fulfilled122 privacy issues may arise in an rct if for example program evaluation captures information about individual citizens or clients that could be used inside the government for purpose other than for wh ich it was collected or released to the public in some form what if any safegu ards should be required of those collecting the information several privacy protections are currently legally required for agencyconducted program evaluations and rcts by the privacy act 5 usc 552a123 among other things the act sets c onditions concerning the disclosure of personally identifiable information pres cribes requirements for the accounting of certain disclosures of the in formation requires agencies to specify their authority and purposes for collecting personally identifia ble information from an individual andhttpwikileaksorgwikicrsrl33301crs39 124 see crs report rl30795 general management laws compendium coordinated by clinton t brass entry for privacy act in ction if of the report by harold c relyea the privacy acts implications for federal pr ogram evaluation should be distinguished from the privacy rule 45 cfr 164 which established set of national standards for the protection of individually identifiable health information the privacy rule restricts the actions of covered entities which are genera lly speaking health care plans and providers for more information on the privacy rule see crs report rl32909 federal protection for human research subjects an analysis of the common rule and its interactions with fda regulations and the hipaa privacy rule by erin d williams appendix 125 the common rule 45 cfr 46 subpart generally requires the informed consent of each participant in federally funded research however agency program evaluations are generally exempt from the rule see 45 cfr 46101b5 the exempted types of research include research designed to study evaluate or otherwise examine public benefit or service programs procedures for obtaining benefits or services under those programs possible changes in or alternatives to those programs or procedures or possible changes in methods or levels of payment for benefits or services under those programs see crs report rl32909 federal protection for human resear ch subjects an analysis of the common rule and its interactions with fda regulations and the hipaa privacy rule by erin d williams 126 ibid the common rule generally requires oversight by an institutional body for federally funded research on human subjects however as noted above agency program evaluations are exempt from the ruleprovides civil and criminal enforcement arrangements124 if program evaluation is funded or directed by an agency but c onducted by nonfederal entity eg if non federal entity creates and ma intains records about progra m evaluation participants the privacy acts coverage is often stated in contracts the issue of access to government programs might arise in an rct if for example the rct were designed with control group that was to be denied access to program as part of the evaluati on although access in some cases might not be required by law its denial raises the stion of whether the be nefits of testing program outweigh the burden of denying access to certain prospective subjects would it be appropriate to design rcts for entitlement programs which guarantee services to clients with such control groups the issue of informed consent might arise in an rct if it were deemed appropriate to enroll only w illing participan ts although informed consent would not always be required by law for many rct s and other types of program evaluation125 its use would guarantee that persons w ho participate in rcts fully understand and agree to their participation would the be nefits of this outwe igh burdens of the time and money that must be spent to achieve such goal if congress chose to ensure that some or all of the above individual protections were implemented in rcts and other form s of program evaluation it might consider requiring some form of protectionsp ecific oversight although probably not required by law for rcts the institutional review of proposed research trials protections for human participants is common requirement for great deal of research126 these reviews generally require th at researchers obtain the approval of an institutional board prior to beginning the research the board checks to makehttpwikileaksorgwikicrsrl33301crs40 127 see lawrence b mohr impact analysis for program evaluation pp 8084 observers have identified many types of contamination and sometimes use different terms to describe them eg spillovers disruptions another majo r threat is attrition of subjects that differs between the treatment and control groupscertain that each proposal includes adequate protections for participants privacy and ensures that the plan to obtain participants informed consent is sufficient among other things though this type of review may be time consumin g and add additional costs to research project it can prove be neficial not only by protecting the studys participants but also by incidentally improving studys design congress and agencies have also instituted other oversight mechanisms for program evaluations including competitions for grants and peer review of grant applications scrutinizing or prospectively assessing studies internal and external validity whether making program ev aluation policy or scrutinizing studies congress might also focus on issues of study interpretation and implementation eg deciding whether to direct or fund evaluations in light of potential contamination risks and projected external validity or judging how much confidence to put in the internal and external validity of study presented during the budget or reauthoriz ation processes contamination and internal validity actors in the policy process will not necessarily advertise any defects in the studies they present to influence congress with that in mind when actors in the policy process present members or committees of congress with program evaluations intended to influence policy decision congress might consider the evaluations realized and not merely theoretical internal validity in addition when cong ress sets program evaluation policy for given policy area it might be possible to prospectively consider the probability of studys successful design implementation and correspondi ng internal validity major threat to the internal validity of rcts and quasiexperiments has been called contamination 127 it should be noted that other designs that attempt to estimate impacts are subject to additional threats incl uding selection bias as noted previously to avoid contamination we lldesigned and implement ed rcts ideally insulate the treatment and control groups from events that might affect one group during the study differently from the other group in way that will affect outcomes doing so is intended to ensure the only systematic difference in the experience of the two groups is whether or not they received the intended treatment rcts also ideally ensure that the intended treatment was admini stered properly because social science research usually does not occur under tig htly controlled laboratory conditions however it is often difficult to insulate study from or control for unforeseen variables that might systematically affect the treatment and cont rol groups differentlyhttpwikileaksorgwikicrsrl33301crs41 128 some illustrations here are drawn from ibid researchers can attempt to design studies ahead of time to avoid contamination they can also make efforts to avoid and monitor contamination during studys implementation for an example regarding study design in an rct of highschool curricula study re searchers might randomize schools instead of individual students ie schools are the randomized units of analysis if one of the schools being studied in the intervention group happened to be in neighborhood that underwent an isolated crime wave the crime wave could have adversely influenced the findings for that group however randomization of schools woul d neutralize the threat of contamination because one would expect schools in the control group to have comparable probability of experiencing crime wavethree examples might help illustrate the threat of contamination128 first if an experiment is not doubleblinded subjects in the treatment group might be aware of their inclusion in special program if this awareness results in psychological effects that are not considered part of the trea tment and subjects beha ve differently the studys results might be contaminated similarly some subjects in control group might learn they are not in the treatment group and either decide to avail themselves on their own initiative of alternative treatme nts that are not associated with the intervention or rese nt or undermine the treatment being given to the other group second if program to curb crime in city were evaluated with an rct certain districts might be the units of analysis perpetrators of crimes might move from experimental districts to control districts contamin ating findings for both groups third with regard to treatment delivery it might be difficult in some studies to ensure that the intended treatment is delivered properly for all subjects or all sites if some subjects in the control group get the treatment for example or if the intended treatment is not delivered properly infe rences about the intended interventions impact might be contaminated in light of these considerations severa l questions might be of concern when congress is presented with program ev aluation findings for example what confidence should congress have that c ontamination did not degrade studys internal validity does the study adequately address these risks also when setting program evaluation policy how much conf idence should congress have that studies in certain policy areas or contexts will be able to avoid contamination what is the track record in given area what are the implications for how congress and agencies should allocate scarce evaluati on resources and structure an agencys portfolio of evaluations generalizability ext ernal validity when congress is presented with program evaluation to justify policy position the program that was evaluated presumably operated at specific time and place and under specific conditions without further analysis to gauge an ev aluations external validity however it will not always be clear whether the intervention itself or the studys findings can be generalized to other circumstances eg future conditions other subjects as noted earlier in this report if generalizability were of concern c ongress might in the first place consider whether the intervention itself ie as it was actually implemented is replicable at different time or place if the interven tion was highly customized to particular time or locale for example an evaluations findings might not be generalizablehttpwikileaksorgwikicrsrl33301crs42 129 however full replication might or might not be appropriate at other times or places if conditions in another environment dictated otherwise 130 if the intervention resulted in favorable impact but was not clearly documented it might not be clear under what conditions the progra m could be expected to result in similar findings if congress were considering expa nding the program conversely if the intervention did not result in favorable im pact and was not clearly documented it might not be clear whether the program would do be tter if implemented under different conditions for example it might be possible that favorable impact could be achieved if the intervention were modified or ta rgeted at different subjectselsewhere unless the intervention were fully replicated in all important respects129 alternatively if an actual intervention were not clearly documented eg how it operated whom it served it might be unclear how to replicate the intervention in that case if the intervention were evaluated it might not be reasonable to expect the evaluations findings would be repeated elsewhere130 with regard to generalizability of findi ngs as opposed to the intervention itself if singlesite rct finds that an interv ention had an impact for group of subjects in one instance it might not necessarily follow that it will have similar impact for other subjects times and ci rcumstances congress might therefore look for multiple impact analysis studies on the subject large scale multisite rcts are often considered more generalizable than singlesite rcts because they estimate an interventions impact in potentially wider array of geographic locations and populations however multisite rcts ar also typically more expensive and difficult to successfully implement compared to singlesite rcts in addition complementary studies are often consider ed necessary to make assessments of generalizability complementary studies might include observational or qualitative evaluations which can potentially be used to better understand an interventions mechanism of causation potential unintende d consequences and conditionality ie the conditions that are required for th intervention to work as intended if these considerations were source of concern congress might scrutinize an evaluations findings for external validity to other times conditions and subjects alternatively if congress is setting evaluation policy for an agency or program or is directing that specific studies occur congress might provide direction or guidance regarding the evaluation methods that mig ht be necessary for establishing programs generalizability to other circumstances issues when directi ng or scrutinizing program evaluations congress might also consider issues that apply to program evaluation generally because an rct is one of many types of program eval uation these issues might be important when congress 1 considers making program evaluation policy for specific agencies or programs eg what type s of evaluations to direct or fund or 2 scrutinizes individual evaluations includi ng rcts when making policy decisions and conducting oversight for example if congress considers legislation that provides for program evaluati on in one or more policy areas to what extent should rcts be the focus of these evaluation po licies to what extent should otherhttpwikileaksorgwikicrsrl33301crs43 evaluation methods be the focus shoul d multiple methods f it into broader evaluation framework the potential issues discussed below raise these and further questions what types of evaluations are necessary given the nature of policy area and the diverse needs of stakeholders including agency program managers the president citizens and notably congress what different types of evaluations should be directed funded nd considered this report began with subsection titled key questions about government pr ograms and policies which outlined number of questions that stak eholders often want to be informed about in response wide array of program evaluation techniques have b een developed certain evaluation types often address or help address multiple kinds of questions indeed many evaluation types are considered complementary to each other at the same time the different types of evaluations bring their own sets of practical capabilities and limitations and experts and practitioners sometimes disagree on the nature and importance of these capabilities and limitations because only finite resources are available for evaluation activities and staff it can be challenging and controve rsial to determine the appropriate methods to be used to help answer certain stakeholder questions fu rthermore when scrutinizing evaluations it can be challenging to discern potential gaps in the perspectives provided by an actor in the policy process for example it can be challenging to discern clues that might suggest other complementar evaluation types are needed given these considerations many issues might be of congre ssional concern for example in congresss view how s hould the executive branch be pursuing evaluations under the part initiative whic h particularly highlighted rcts how should congress oversee and respond to the ad ministrations efforts to achieve budget and performance integration throug h use of the part in the education policy arena to what extent is ed appropriately implementing the program evaluation aspects of nclb and esra to what extent is the ed priority which elevated rcts and quasiexper iments above other evaluati on types consistent with congressional intent how is ed implementing the priority as congress considers legislation in other policy areas shoul d congress provide direction or guidance regarding program evaluation policy or methods if so what kinds of studies might agencies congress and outside stakehol ders need when actors in the policy process present evaluations to influence congress are the actors presenting the full story or are there gaps in the presentation are the evaluations they present capturing the key questions that need to be answered any of these multiple questions might be ripe for attention what definitions and assu mptions are being used as noted earlier many actors in the policymaking process use program evaluations to help justify their policy recommendations and to attempt to persuade congre ss to make decisions consistent with their policy objectives th erefore many observers have considered it important that policy makers including members and committees of congress be informed consumers of evaluation info rmation unfortunately however the vocabulary of program evaluation can sometimes be confusing for example many observers and practitioners use the same terms but with differing definitions forhttpwikileaksorgwikicrsrl33301crs44 those terms when someone says program is effective in what sense is the term being used as noted previously the term effectiveness might refer to 1 programs overall merit or worth 2 the extent to which program is accomplishing its goals 3 the programs impact on particular outcome of interest or 4 an ambiguous mix of all three prior definitions these are very different concepts in any of these senses determining whethe r program is effective can hinge upon an observers views and assumptions bout the programs mission objectives appropriate outcomes of in terest and progress thus should these definitional aspects of program evaluations be of concern congress might scrutinize them closely furthermore assumptions that are implicit in an evaluation might go unstated some stakeholders or evaluators might implicitly argue that their preferred way of evaluating program is best and th at other methods are comparatively inappropriate or less appropriate howe ver there is not always consensus among wellrespected experts regarding when certain methods are best or most appropriate should these matters be of concern to congress when they occur congress might investigate number of questions for ex ample if one method is claimed to be best compared to others what are the stated and unstated reasons for that opinion what would other stakeholders and evaluators say to what extent if any might opinions of appropriate ness be due to an underlying agenda eg to support policy views or selfinterest eg to get funding for program or type of evaluation these questions might also be applied to the cases discussed in this report for example in justifying the ed priority for rcts which claimed rcts are best for determining effectiveness which definition for effectiveness is ed using is the ed definition consistent with how ed intends to use rcts should ed employ additional or alternative types of evaluati on for these intended uses of rcts in its strategic planning budgetin g and operations is ed using multiple definitions what definition is being used for overall effectiveness for purposes of the part how should congress use evaluation information when considering and making policy when congress is presented with evaluation information in the policy process by vari ous actors eg lobbyists experts think tanks academics or agencies among others how should congress use the information and findings there is wide spread consensus that program evaluations can help policy makers gain insights into policy problems and make betterinformed decisions regarding ways to improve government performance transparency accountability and efficiency nonetheless the use of evaluation in the policy making process can be controversial for example some advocates of performancebased budgeting and evidence based policy have argued that programs fu ture funding or existence should be based on its performance or on eviden ce of its effectiveness the terms performancebased budgeting and evidencebased policy however do not have consensus definitions because different act ors typically have among other things different definitions for what constitute s performance and ev idence conceptions of what it means to base decision maki ng on performance or evidence views abouthttpwikileaksorgwikicrsrl33301crs45 131 past efforts to use performance informa tion evidence and analysis to drive policy decision making eg planningprogrammingbudgeting systems in the 1960s and zero based budgeting in the 1970s were similarly subject to uncertainties relating to whether how and to what extent evidence or analysis can be used to drive or influence complex decisions these decisions are usually made about priorities policy changes and resource allocation in the face of tradeoffs uncertain ty and limited information all of which are subject to diverse views and conflicting values in response to these kinds of issues some observers have proposed dropping based from terms name and replacing it with informed eg performanceinformed budgeting the term performancebased budgeting appears to have its postworld war ii roots in the work of the first hoover commission for discussion see crs report rl32164 performance management and budgeting in the federal government brief hi story and recent developments by virginia mcmurtry the term evidencebased policy appears to have its roots in evidencebased practice term typically concerning behavioral health disciplin such as psychiatry and social work and ultimately evidencebased medicine for discussion of the latter two terms see richard n rosenthal overview of evidencebased prac tice in albert r roberts and kenneth r yeager eds evidencebased practice manual research and outcome measures in health and human services new york oxford university press 2004 pp 2029 132 this has been described as bounded rationality see herbert simon administrative behavior study of deci sionmaking processes in administrative organization 3rd ed new york free press 1976 pp 240247whether decision should be based on past performance or evidence and views about what other factors should leg itimately help drive decision making131 another fundamental question might be of concern to policy makers and stakeholders what ro should evaluation of past events play in forming future strategies and plans prominent argument in favor of using evaluations to shape future strategies is that past performance by person program agency etc is usually the best predictor of future pe rformance however prominent counter argument is that focusing primarily on the past can be compared to driving car using only the rearview mirror although evaluation of past performance is widely considered helpful for informing thinking and decisions the process of strategic decision making has been found in the soci science and mana gement literatures to be legitimately driven by many more factors these have included among others basic and applied research forecasting nd scenario planning risk assessment professional judgment from individua l and group experience theoretical extrapolation intuition especially when information is incomplete consensus interpretations of information are lacking the future is uncertain or synthesis is necessary and values in addition in spite of efforts to make decisions as rational as possible in the face of uncertainty nd limited information wide body of social science has found that there are practical limits to rationality in decision making132 should these considerations be of cern to congress when considering policy questions or conducting oversig ht several questions might be asked for example in an rct context when considering or scrutinizing rct studies how should these studies be used by congress to inform thinking and decision making how should they be used by agencies and omb are agencies and omb using them in appropriate ways what other fact ors can or should be consideredhttpwikileaksorgwikicrsrl33301crs46 133 for example the internal revenue servic has multiple goals that are implicit in its mission statement including service enforcem ent and fairness different stakeholders might disagree regarding which of these implicit goals is the most important furthermore there might be implicit tradeoffs among the goals that prevent simultaneous maximization of all three for more on irss mission and goals see us department of the treasury internal revenue service irs strategic plan 20052009 washington 2004 available at http wwwirsgovpub irsutlstrategic_plan_0509pdf 134 certain kinds of information disseminated by federal agency often including program evaluations might be covered by statutory provisions informally called the information quality act section 515 of the fy2001 treasury and general government appropriations act 114 stat 2763a153 under omb guidelines agencies are required to issue their own guidelines to ensure and maximize the qua lity objectivity utility and integrity of certain kinds of disseminated information the information quality guidelines of the department of health and human services hhs cove r program evaluations among other things within their scope see httpaspehhsgovin foqualityguidelinespart1shtml see also crs report rl32532 the information quality act ombs guidance and initial implementation by curtis w copeland and michael simpson how much confidence should one have in study in order to inform ones thinking and decisions if congress is presented in decision making situation with single program evaluation is the evaluation enough to have high confidence in the findings progra m evaluations can provide helpful insight into policy problems and the manner and ex tent to which federal policies address those problems unfortunately however one or more program evaluations do not always produce information that is compre hensive accurate credible or unbiased for example program evaluation can be designed to answer certain question but the way someone frames the original evaluation question can influence how program is ultimately portrayed most prominently this can be the case when setting criteria for success such as programs goals or the preferred outcomes of interest however actors in the policy process ofte n have varying views on how to judge programs or policys success133 it is not always clear therefore that studys research question will be viewed by most observers as covering what should have been covered to validly or comp rehensively evaluate program in addition program evaluation will not always necessarily be well designed or implemented in such cases study might produce results that are flawed or inaccurate even in the best case if an evaluation is appropriate for the research question being studied is well designed and implemented and there is widespread consensus on how to judge success it is still possible that random chance or unforeseen events might result in an evaluation that produces information that is inaccurate or flawed for example it is possible that an evaluation might provide false positive or false negative result g the study finds the program successful when actually it was not or finds pr ogram unsuccessful when it actually was successful if this situation were the case the study findings might not reveal it the subject of data or information quality is also oftentimes subject of concern when conducting or interpre ting program evaluations134 how might congress cope with these possibilities how confident must member or committee be in evaluation info rmation including from rcts in orderhttpwikileaksorgwikicrsrl33301crs47 135 us general accounting office program evaluation agencies challenged by new demand for informati on on program results gao ggd9853 apr 1998 p 1 and performance budgeting opportunities and challenges gao021106t sept 2002 p 16 see also us government accountability office program evaluation ombs part reviews increased agencies attention to improving evidence of program results gao 0667 pp 1516 28 136 evaluations of federal programs are also c onducted by outside contr actors nevertheless because program evaluations are undertaken in these cases under the oversight of agencies and presumably interpreted by agency leaders and analytical staffs ie because agencies are consumers of evaluation information the agencies might need analytical competency in several types of program evaluationto use the information to inform thinking and conclusions about policy in response social science re searchers have recommended that consumers of evaluation information be aware of the practical capabilities and limitations of various program evaluation methods and also scrutinize st udys claims of internal external and construct validity they have also sugge sted looking for mu ltiple studi and if available systematic review s other observers have suggested using the resources of gao or other congressional support agen cies to help interpret or validate conclusions and scrutinizing these matters through hearings and oversight finally congress might consider whether federal agencies have sufficient capacity and independence to conduct inte rpret and objectively pres ent program evaluations to congress do agencies have capacity and independence to properly conduct interpret and objectiv ely present program evaluations at times congress and other actors have expressed concern over the capacity of agencies to adequately perform certain ta sks including management functions that range from procurement to financial management on management function that has been frequently cited as topic of c oncern is program evaluation over long period of time gao has f ound limited and diminishing resources spent on program evaluation and reason to be c oncerned about the capacity of federal agencies to produce evaluations of their programs effectiveness135 even with recent emphasis on program evaluation under gpra and the bush administrations part it is unclear the extent to which agencies have capacity to properly conduct interpret or use program evaluations136 many if not all of the issues discussed in this report could appl equally to organizations and decision makers within federal agencies should program evaluation capacity in fede ral agencies be seen as topic of concern several questions mig ht be considered by congress given the complex issues and debates involved in the produc tion interpretation and use of program evaluations as well as complex debate s about the appropria teness of different evaluation types in certain circumstances do agencies have capacity to use evaluation information to soundly inform strategic and operational decisions in addition do agencies have the capacity to make objective methodologically sound presentations and interpretations of eval uation information to congress including information from rcts httpwikileaksorgwikicrsrl33301crs48 137 according to omb guidance the part requires evaluations to be independent conducted by nonbiased parties with conflict of interest omb interpreted this guidance to not allow program evaluations to be conducted by programs themselves for purposes of the part instead omb allows contractedout ev aluations by third parties to count for the part and said that evaluations by inspectors general and agency program evaluation offices might also be considered independent see us office of management and budget guidance for completing the pr ogram assessment rating tool mar 2005 pp 45 available at ombs website http wwwwhitehous egovombpartindexhtml under the link instructions for completing the 2005 part at httpwwwwhitehousegovombpartfy20052005_guidancepdf for discussion of disagreements between agencies and omb bout part independence requirements see us government accountability office program evaluation ombs part reviews increased agencies attention to im proving evidence of program results pp 2526 it should be noted that omb and the part its elf would not necessarily be considered independent under these criteria because am ong other things the administration and omb sometimes established the goals and performance measures by which programs would be evaluated under the part 138 see 42 usc 299b as amended by pl 106129 the healthcare research and quality act of 1999 113 stat 1653 139 ahrqs website says the agencys main f unctions are to sponsor and conduct research that provides evidencebased information on health care outcomes quality and cost use and access to help health care decisionmakers patients and clinicians health system leaders purchasers and policymakers make more informed decisions and improve the quality of health care services for more bout the agencys mission customers and goals see httpwwwahrqgovaboutprofilehtm furthermore do agency program eval uation offices and personnel have the necessary independence from politics eg partisan or institutional and selfinterest to without undue hindrance raise potentia lly uncomfortable issues and surface objective valid and reliable findings for consideration by po licy makers including congress137 should they have this kind of independence finally and more broadly congress might consider issues of evaluation capacity that go beyond federal programs in the past congress has established agencies that focus on evaluation issues in entire policy areas for example in 1989 congress established new agency within the department of health and human services to serve as focal point in hea lth care research congress reauthorized the agency now called the agency for healthcare research and quality ahrq in 1999138 rather than focus only on evaluatin g federal programs ahrqs statutory mission is to conduct and support research in all aspects of health care synthesis and dissemination of available scientific evidence for use by multiple stakeholders and initiatives to advance health care quality139 as noted previously in this report congress also established ies within ed in 2002 ies has multipart mission to provide national leadership in expandi ng fundamental knowle dge and understanding in education from early childhood t hough postsecondary study for many stakeholders providing them with relia ble information about the condition and progress of education in the united stat educational practices that support learning and improve acade mic achievement and access to educational opportunities for all students and the effectiveness of federal and other education programs 116 stat 1944 httpwikileaksorgwikicrsrl33301crs49 although there are differences in the missi ons of these agenci one aspect that arguably makes them similar is the scope of their research and evaluation work specifically their focus goes beyond federal programs to instead encompass research and evaluations throughout an entire policy ar ea such as education or health care whether interventions are delivered by the federal government or another entity should congress view program evaluation cap acity as an issue for an entire policy area congress could move to consider whether establishment of policy research and evaluation agency might be warrantedhttpwikileaksorgwikicrsrl33301crs50 appendix glossary of selected terms and concepts the vocabulary of program evaluation unfortunately for consumers of eval uation information the vocabulary of program evaluation can sometimes be technical and difficult the field is multi disciplinary and some concepts are complex sometimes it is not always clear in what sense term is being used and whet her the term is being used appropriately technical experts and actors in the policy process sometimes use the same terms for different concepts or use di fferent terms for the same concept nonetheless in program evaluations understanding the mean ings of and distinctions between key terms can make significant difference in how to interpret study findings and limitations and in how to scrutinize evaluations to see if they are being represented objectively and forthrightly this appendix draws on the report to briefly define and if necessary explain several recurring terms and to briefly identify several definitions for the same term as appropriate however the definitions provided below are illustrative only and do not necessarily indicate what an evaluations author or what an actor in the policy process intends to communicate more definitive assessments typically must be made on casebycase basis the footnotes in this report provide written resources that can help with understanding evalua tions and the terms they employ and crs analysts can provide additional assistance or refer readers to other resources in each entry term that is included elsewhere in the glossary is written in italics the first time it is used selected terms and concepts construct validity in practice there are several definitions of this term 1 in measuring outcomes the extent to which study actually evaluates what it is being represented as evaluating eg does the studys outcome of interest actually measure student achievement and 2 in relation to program the extent to which the actual program reflects ones ideas and theo ries of how the program is supposed to operate and the causal mechanism through which it is supposed to achieve outcomes contamination in an rct something aside from the intended treatment that might affect the treatment group or control group differently from the other group in way that will affect observed outcomes for example rcts should ideally insulate the treatment and control groups from contaminating events in order to ensure that the only difference in the expe rience of the two groups is whether or not they received the intended treatment in addition rcts should ideally ensure that the treatment was administered properly otherwise the treatment might be considered contaminated control group in an rct group of subjects chosen by random assignment that is comparable to the treatment group but that does not experience the program being studiedhttpwikileaksorgwikicrsrl33301crs51 effect depending on usage something that inevitably follows an antecedent as cause or agent for example the act of dropping pen cause is closely followed by noise an effect when the pen strikes the floor also used sometimes as synonym for impact effective effectiveness term with multiple possible definitions in practice it is used as synonym for impact merit and worth or accomplishment of specific intended goals evaluation an applied inquiry process for collecting and synthesizing evidence that culminates in conclusions about the state of affairs value merit worth significance or quality of program product person policy proposal or plan external validity the extent to which an intervention being studied can be 1 applied to and replicated in other ttings times or groups of subjects and 2 expected to deliver similar impact on an outcome of interest the terms generalizability replicability and repeatab ility are sometimes used as synonyms for external validity some usage of this term refers only to one of the two aspects noted here government performance and results act gpra of 1993 federal law that requires most executive branch agencies to develop fiveyear strategic plans annual performance plans including goals and performance indicat ors among other things and annual program pe rformance reports in gpra s legislative history it was contemplated that not all forms of program evaluation and measurement would necessarily be quantifiable because of the large diversity of federal government activities impact an estimated measurement of how program intervention affected the outcome of interest for large group of subjects on average compared to what would have happened without the intervention for example if the unemployment rate in geographic area would have b een 6 without an intervention but was estimated to be 5 because of the interv ention the impact would be 1 reduction in the unemployment rate ie 6 minus 5 equals an impact of 1 or alternatively 167 reducti on in the unemployment rate if one characterizes the impact as proportion of the prior unemp loyment rate depending on the chosen outcome of interest the average impact across all subjects usually reflects the weighted average of the subjects who experienced favorable impacts subjects who did not experience change and others who experienced unfavorable impacts some theorists and practitioners use the term effect as synonym for impact internal validity in an rct the confidence with which one can state that the impact found or inferred by study was cause d by the intervention being studied merit the overall intrinsic value of program to individuals this term is usually paired with the term worth httpwikileaksorgwikicrsrl33301crs52 metaanalysis type of systematic review that uses statistical methods to derive quantitative results from the analysis of multiple sources of quantitative evidence observational design term that has been used in different ways but that often refers to empirical and qualitative evaluations of many types that are intended to help explain causeandeffect relations hips but do not attemp t to approximate an rct outcome of interest something oftentimes public policy goal that one or more stakeholders care about eg unemployment rate wh ich many actors might like to be lower there can be many potential outcomes of interest related to program actors in the policy process will not necessarily agree which outcomes are important outcomes of government programs need not always be quantitative eg sending humans safely to the moon and back to earth performance measurement performance measure term that can mean many things but is usually considered to be different from program evaluation typically the term refers to ongoing and periodic monitoring and reporting of program operations or accomplishments eg progress toward quantitative goals and sometimes also statistical information related to but not necessarily influenceable by program occasi onal synonyms for measure are indicator metric and target some times the word performance is dropped especially when stakeholders believe the measure does not necessarily indicate whether the program itself caused changes in favorable or unfavorable directions program government policy activity project initiative law tax provision function or set thereof that some one might wish to evaluate in program evaluation synonyms for program include treatment and intervention program evaluation under the government performance and results act gpra of 1993 an assessment through objective measurement and systematic analysis of the manner and extent to which federal progra ms achieve intended objectives program evaluation has been seen as 1 informing conclusions at particular points in time and also 2 cumulative process over time of forming conclusions as more evaluation information is collected and interpreted practitioners and theorists categorize different types of program evaluation in several ways the varying types are sometimes referre d to generically as designs or methods typical synonyms for this term include evaluation and study qualitative evaluation wide variety of evaluation types that judge the effectiveness of program eg whether it accomplishes its goals by among other things conducting opene nded interviews directly observing program implementation and outcomes reviewing documents and constructing case studies quasiexperimental design type of evaluation that attempts to estimate treatments impact on an outcome of interest for group of subjects but in contrast with rcts does not have random assignment to treatment and control groups some quasiexperimental desi gns are controlled studies ie with control group andhttpwikileaksorgwikicrsrl33301crs53 at least one treatment group but othe rs lack control group some quasi experiments do not measure the outcome of interest before the treatment takes place some observers and practitioners consider quasiexperiments to be form of observational design but others put them in their own category random assignment the process of assigning subjects into control group and one or more treatment groups by random chance random selection the process of drawing sample by random chance from larger population eg to undertake survey that is intended to be representative of broader population this term is different from and sometimes confused with random assignment randomized controlled trial rct in its basic form an evaluation design that uses random assignment to assign some subjects to treatment group and also to control group the treatment group participates in the program being evaluated and the control group does not after the treatment group experiences the intervention an rct attempts to comp are what happens to the two groups as measured by the resulting difference between the two groups on the outcome of interest in order to estimate the programs impact the terms randomized field trial rft random assignment design exper imental design random experiment and social experiment are sometimes used as synonyms for rct and vice versa use of the word field in this context is often intended to imply that an evaluation is being conducted in more naturalis tic setting instead of labor atory or other artificial environment doubleblind studies are t hose in which neither the subjects nor the researchers know which group gets the trea tment singleblind studies are those in which the subjects do not know they are ge tting the treatment being investigated statistical significance in the context of an rct finding of statistical significance is typically interpreted as level of confidence usually expressed as probability eg 95 which is also referred to as signi ficance at the 05 level that an estimated impact is not merely the result of random variation assuming the rct suffered from defects this finding woul d indicate that at least some of the measured impact may with substantia l confidence eg 95 confidence be attributed to the treatment as cause stat ed another way significance at the 05 level indicates that there is 1 in 20 chance that the observed difference could have occurred by chance if the program actually had impact however simply because an estimated impact is found to be statisti cally significant does not necessarily mean the impact is large or important systematic review form of structured literature review that addresses question that is formulated to be answer ed by analysis of evidence and involves objective means of searching the literature applying predetermined inclusion and exclusion criteria to this literature critically appraising the relevant literature and extraction and synthesis of data from the evidence base to formulate findings although systematic reviews typically focus much attention on concerns about internal validity of various studies judgments about external validity or generalizability of findings are often left to readers to assess based on their implicit or explicit decision how applicable the systematic reviews evidence is to theirhttpwikileaksorgwikicrsrl33301crs54 particular circumstances in program evaluation systematic reviews have been performed under various names eg eval uation synthesis integrative review research synthesis in different ways nd usually in decentralized fashion some systematic reviews focus on rcts and might include quasiexperiments and others include disparate types of studies treatment group i n n rct the group of subjects chosen by random assignment that experiences or participates in program also sometimes called an experimental or intervention group unit of analysis in an rct the subjects of the study who are randomly assigned to one or more treatment groups and also control group subjects are typically individual persons but sometimes might be things or organizations like schools hospitals or police stations validity see entries for internal validity external validity and construct validity worth the overall extrinsic value of program to society this term is usually paired with the term merit