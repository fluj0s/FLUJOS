from ben santer santer1llnlgov to thomasrkarl thomasrkarlnoaagov subject re more significance testing stuff date wed 02 jan 2008 205231 0800 replyto santer1llnlgov cc johnlanzantenoaagov carl mears mearsremsscom david c bader bader2llnlgov dian j seidel dianseidelnoaagov francis w zwiers franciszwiersecgcca frank wentz frankwentzremsscom karl taylor taylor13llnlgov leopold haimberger leopoldhaimbergerunivieacat melissa free melissafreenoaagov michael c maccracken mmaccraccomcastnet philip d jones pjonesueaacuk sherwood steven stevensherwoodyaleedu steve klein klein21llnlgov susan solomon susansolomonnoaagov thorne peter peterthornemetofficegovuk tim osborn tosbornueaacuk tom wigley wigleycgducaredu gavin schmidt gschmidtgissnasagov xflowed dear tom in the end i decided to test the significance of trends in the ot minus mt difference time series as you and john lanzante have suggested i still think that this difference series test is more appropriate when one is operating on pair of time series with correlated variability for example if you wished to test whether an observed tropical t2lt trend was significantly different from the t2lt trend simulated in an amip experiment but you and john convinced that our response to douglass et would be strengthened by using several different approaches to address the statistical significance of differences between modeled and observed temperature trends the tables given below show the results from two different types of test youve already seen the type1 or paired trend results these involve bo and bm which represent any single pair of observed and modeled trends with standard errors sbo and sbm which are adjusted for temporal autocorrelation effects as in our previous work and as in related work by john lanzante we define the normalized trend difference d as d1 bo bm sqrt sbo2 sbm2 under the assumption that d1 is normally distributed values of d1 196 or 196 indicate observedminusmodel trend differences that are significant at the 5 level and one can easily calculate pvalue for each value of d these pvalues for the 98 pairs of trend tests 49 involving uah data and 49 involving rss data are what we use for determining the total number of hits or rejections of the null hypothesis of significant difference between modeled and observed trends i note that each test is twotailed since we have information priori about the direction of the model trend ie whether we expect the simulated trend to be significantly larger or smaller than observed the type2 results are the difference series tests these involve ot and mt which represent any single pair of modeled and observed layeraveraged temperature time series one first defines the difference time series dt ot mt and then calculates the trend bd in dt and its adjusted standard error sbd the test statistic is then simply d2 bd sbd as in the case of the paired trend tests we assume that d2 is normally distributed and then calculate pvalues for the 98 pairs of difference series tests as i mentioned in previous email the interpretation of the difference series tests is little complicated over half 35 of the 49 model simulations examined in the ccsp report include some form of volcanic forcing in these 35 cases differencing the ot and mt time series reduces the amplitude of this externallyforced component in dt this will tend to reduce the overall temporal variability of dt and hence reduce sbd the standard error of the trend in dt such noise reduction should make it easier to identify true differences in the anthropogenicallyforced components of bo and bd but since the internallygenerated variability in ot and mt is uncorrelated differencing ot and mt has the opposite effect of amplifying the noise thus inflating sbd and making it more difficult to identify modelversusobserved trend differences the results given below show that the paired trend and difference series tests yield very similar rejection rates of the null hypothesis the bottom line is that regardless of which test we use which significance level we stipulate which observational dataset we use or which atmospheric layer we focus on there is evidence to support douglass et als assertion that all uah and rss satellite trends are inconsistent with model results rejection rates for stipulated 5 significance level test type of tests t2 hits t2lt hits 1 obsvsmodel type1 49 x 2 98 2 204 1 102 2 obsvsmodel type2 49 x 2 98 2 204 2 204 rejection rates for stipulated 10 significance level test type of tests t2 hits t2lt hits 1 obsvsmodel type1 49 x 2 98 4 408 2 204 2 obsvsmodel type2 49 x 2 98 3 306 3 306 rejection rates for stipulated 20 significance level test type of tests t2 hits t2lt hits 1 obsvsmodel type1 49 x 2 98 7 714 5 510 2 obsvsmodel type2 49 x 2 98 10 1020 7 714 as ive mentioned in previous emails i think its little tricky to figure out the null distribution of rejection rates ie the distribution that might be expected by chance alone my gut feeling is that this is easiest to do by generating distributions of the d1 and d2 statistics using model control run data only use of monte carlo procedures gets into issues of whether one should use block resampling and attempt to preserve the characteristic decorrelation times of the model and observational data being tested etc etc thanks very much to all of you for your advice and comments i still believe that there is considerable merit in brief response to douglass et i think this could be done relatively quickly from my perspective this response should highlight four issues 1 it should identify the flaws in the statistical approach used by douglass et to compare modeled and observed trends 2 it should do the significance testing properly and report on the results of paired trend and difference series tests 3 it should show something similar to the figure that leo recently distributed ie zonalmean trend profiles in various versions of the raobcore data and highlight the fact that the structural uncertainty in sondebased estimates of tropospheric temperature change is much larger than was claimed in douglass et 4 it should note and discuss the considerable body of complementary evidence supporting the finding that the tropical lower troposphere has warmed over the satellite era with best regards ben thomasrkarl wrote thanks ben you have been busy i sent tom an email before reading the last paragraph of this note recognizing the random placement of enso in the models and volcanic effects in few and the known impact of the occurrence of these events on the trends i think it is appropriate that the noise and related uncertainty about the trend differences be increased amplifying the noise could be argued as an appropriate conservative approach since we know that these events are confounding our efforts to see differences between models and obs wr to greenhouse forcing i know it is more work but i think it does make sense to calculate o1m1 o2m2 onmn for all combinations of observed data sets and model simulations you could test for significance by using monte carlo bootstrap approach by randomizing the years for both models and data regards tom ben santer said the following on 12262007 950 pm dear john thanks for your email as usual your comments were constructive and thoughtprovoking ive tried to do some of the additional tests that you suggested and will report on the results below but first lets have brief recap as discussed in my previous emails ive tested the significance of differences between trends in observed msu time series and the trends in synthetic msu temperatures in multimodel ensemble of opportunity the ensemble of opportunity comprises results from 49 realizations of the cmip3 20c3m experiment performed with 19 different aogcms this is the same ensemble that was analyzed in chapter 5 of the ccsp synthesis and assessment product 11 ive used observational results from two different groups rss and uah from each group we have results for both t2 and t2lt this yields total of 196 different tests of the significance of observedversusmodel trend differences 2 observational datasets x 2 layeraveraged temperatures x 49 realizations of the 20c3m experiment thus far ive tested the significance of trend differences using t2 and t2lt data spatially averaged over oceans only both 20n20s and 30n30s as well as over land and ocean 20n20s all results described below focus on the land and ocean results which facilitates direct comparison with douglass et here was the information that i sent you on dec 14th combined landocean results with standard errors adjusted for temporal autocorrelation effects spatial averages over 20n20s analysis period 1979 to 1999 t2lt tests rss observational data 0 out of 49 modelversusobserved trend differences are significant at the 5 level t2lt tests uah observational data 1 out of 49 modelversusobserved trend differences are significant at the 5 level t2 tests rss observational data 1 out of 49 modelversusobserved trend differences are significant at the 5 level t2 tests uah observational data 1 out of 49 modelversusobserved trend differences are significant at the 5 level in other words at stipulated significance level of 5 for twotailed test we rejected the null hypothesis of significant difference between observed and simulated tropospheric temperature trends in only 1 out of 98 cases 102 for t2lt and 2 out of 98 cases 204 for t2 you asked john how we might determine baseline for judging the likelihood of obtaining the observed rejection rate by chance alone you suggested use of bootstrap procedure involving the model data only in this procedure one of the 49 20c3m realizations would be selected at random and would constitute the surrogate observations the remaining 48 members would be randomly sampled with replacement 49 times the significance of the difference between the surrogate observed trend and the 49 simulated trends would then be assessed this procedure would be repeated many times yielding distribution of rejection rates of the null hypothesis as you stated in your email the actual number of hits based on the real observations could then be referenced to the monte carlo distribution to yield probability that this could have occurred by chance one slight problem with your suggested bootstrap approach is that it convolves the trend differences due to internallygenerated variability with trend differences arising from intermodel differences in both climate sensitivity and in the forcings applied in the 20c3m experiment so the distribution of hits as you call it or rejection rates in my terminology is not the distribution that one might expect due to chance alone nevertheless i thought it would be interesting to generate distribution of rejection rates based on model data only rather than implementing the resampling approach that you suggested i considered all possible combinations of trend pairs involving model data and performed the paired difference test between the trend in each 20c3m realization and in each of the other 48 realizations this yields total of 2352 49 x 48 nonidentical pairs of trend tests for each layeraveraged temperature time series here are the results t2 at stipulated 5 significance level 58 out of 2352 tests involving model data only 247 yielded rejection of the null hypothesis of significant difference in trend t2lt at stipulated 5 significance level 32 out of 2352 tests involving model data only 136 yielded rejection of the null hypothesis of significant difference in trend for both layeraveraged temperatures these numbers are slightly larger than the observed rejection rates 204 for t2 and 102 for t2lt i would conclude from this that the statistical significance of the differences between the observed and simulated msu tropospheric temperature trends is comparable to the significance of the differences between the simulated 20c3m trends from any two cmip3 models with the proviso that the simulated trend differences arise not only from internal variability but also from intermodel differences in sensitivity and 20th century forcings since i was curious i thought it would be fun to do something little closer to what you were advocating john ie to use model data to look at the statistical significance of trend differences that are not related to intermodel differences in the 20c3m forcings or in climate sensitivity i did this in the following way for each model with multiple 20c3m realizations i tested each realization against all other nonidentical realizations of that model eg for model with an 20c3m ensemble size of 5 there are 20 paired trend tests involving nonidentical data i repeated this procedure for the next model with multiple 20c3m realizations etc and accumulated results in our ccsp report we had access to 11 models with multiple 20c3m realizations this yields total of 124 paired trend tests for each layeraveraged temperature time series of interest for both t2 and t2lt none of the 124 paired trend tests yielded rejection of the null hypothesis of significant difference in trend at stipulated 5 significance level you wanted to know john whether these rejection rates are sensitive to the stipulated significance level as per your suggestion i also calculated rejection rates for 20 significance level below ive tabulated comparison of the rejection rates for tests with 5 and 20 significance levels the two rows of modelvsmodel results correspond to the two cases ive considered above ie tests involving 2352 trend pairs row 2 and 124 trend pairs row 3 note that the observedvsmodel row row 1 is the combined number of hits for 49 tests involving rss data and 49 tests involving uah data rejection rates for stipulated 5 significance level test type of tests t2 hits t2lt hits row 1 observedvsmodel 49 x 2 2 204 1 102 row 2 modelvsmodel 2352 58 247 32 136 row 3 modelvsmodel 124 0 000 0 000 rejection rates for stipulated 20 significance level test type of tests t2 hits t2lt hits row 1 observedvsmodel 49 x 2 7 714 5 510 row 2 modelvsmodel 2352 176 748 100 425 row 3 modelvsmodel 124 8 645 6 484 so what can we conclude from this 1 irrespective of the stipulated significance level 5 or 20 the differences between the observed and simulated msu trends are on average substantially smaller than we might expect if we were conducting these tests with trends selected from purely random distribution ie for the row 1 results 204 and 102 5 and 714 and 510 20 2 why are the rejection rates for the row 3 results substantially lower than 5 and 20 shouldnt we expect if we are only testing trend differences between multiple realizations of the same model rather than trend differences between models to obtain rejection rates of roughly 5 for the 5 significance tests and 20 for the 20 tests the answer is clearly the row 3 results do not involve tests between samples drawn from population of randomlydistributed trends if we were conducting this paired test using randomlysampled trends from long control simulation we would expect given sufficiently large sample size to eventually obtain rejection rates of 5 and 20 but our row 3 results are based on paired samples from individual members of given models 20c3m experiment and thus represent both signal response to the imposed forcing changes and noise not noise alone the common signal component makes it more difficult to reject the null hypothesis of significant difference in trend 3 your point about sensitivity to the choice of stipulated significance level was welltaken this is obvious by comparing row 3 results in the 5 and 20 test cases 4 in both the 5 and 20 cases the rejection rate for paired tests involving modelversusobserved trend differences row 1 is comparable to the rejection rate for tests involving intermodel trend differences row 2 arising from the combined effects of differences in internal variability sensitivity and applied forcings on average therefore model versus observed trend differences are not noticeably more significant than the trends between any given pair of cmip3 models nb this inference is not entirely justified since row 2 convolves the effects of both intermodel differences and within model differences arising from the different manifestations of natural variability superimposed on the signal we would need row 4 which involves 19 x 18 paired tests of model results using only one 20c3m realization from each model ill generate row 4 tomorrow john you also suggested that we might want to look at the statistical significance of trends in time series of differences eg in ot minus mt or in m1t minus m2t where denotes observations and m denotes model and t is an index of time in months while ive done this in previous work for example in the santer et 2000 jgr paper where we were looking at the statistical significance of trend differences between multiple observational upper air temperature datasets i dont think its advisable in this particular case as your email notes we are dealing here with aogcm results in which the phasing of ninos and ninas and the effects of enso variability on t2 and t2lt differs from the phasing in the real world so differencing mt from ot or m2t from m1t probably actually amplifies rather than damps noise particularly in the tropics where the externallyforced component of mt or ot over 1979 to 1999 is only relatively small fraction of the overall variance of the time series i think this amplification of noise is disadvantage in assessing whether trends in ot and mt are significantly different anyway thanks again for your comments and suggestions john they gave great opportunity to ignore the hundreds of emails that accumulated in my absence and instead do some science with best regards ben john lanzante wrote ben perhaps resampling test would be appropriate the tests you have performed consist of pairing an observed time series uah or rss msu with each one of 49 gcm times series from your ensemble of opportunity significance of the difference between each pair of obsgcm trends yields certain number of hits to determine baseline for judging how likely it would be to obtain the given number of hits one could perform set of resampling trials by treating one of the ensemble members as surrogate observation for each trial select at random one of the 49 gcm members to be the observation from the remaining 48 members draw bootstrap sample of 49 and perform 49 tests yielding certain number of hits repeat this many times to generate distribution of hits the actual number of hits based on the real observations could then be referenced to the monte carlo distribution to yield probability that this could have occurred by chance the basic idea is to see if the observed trend is inconsistent with the gcm ensemble of trends there are couple of additional tweaks that could be applied to your method you are currently computing trends for each of the two time series in the pair and assessing the significance of their differences why not first create difference time series and assess the significance of its trend the advantage of this is that you would reduce somewhat the autocorrelation in the time series and hence the effect of the degrees of freedom adjustment since the gcm runs are based on coupled model runs this differencing would help remove the common externally forced variability but not internally forced variability so the adjustment would still be needed another tweak would be to alter the significance level used to assess differences in trends currently you are using the 5 level which yields only small number of hits if you made this less stringent you would get potentially more weaker hits but it would all come out in the wash so to speak since the number of hits in the monte carlo simulations would increase as well i suspect that increasing the number of expected hits would make the whole procedure more powerfulefficient in statistical sense since you would longer be dealing with rare event in the current scheme using 5 level with 49 pairings you have an expected hit rate of 005 x 49 245 for example if instead you used 20 significance level you would have an expected hit rate of 020 x 49 98 i hope this helps on an unrelated matter im wondering bit about the different versions of leos new radiosonde dataset raobcore i was surprised to see that the latest version has considerably more tropospheric warming than i recalled from an earlier version that was written up in jcli in 2007 i have couple of questions that id like to ask leo one concern is that if we use the latest version of raobcore is there paper that we can reference if this is not in peerreviewed journal is there paper in submission the other question is could you briefly comment on the differences in methodology used to generate the latest version of raobcore as compared to the version used in jcli 2007 and whatwhenwhere did changes occur to yield stronger warming trend best regards ______john on saturday 15 december 2007 1221 pm thomasrkarl wrote thanks ben you have the makings of nice article i note that we would expect to 10 cases that are significantly different by chance based on the 196 tests at the 05 sig level you found 3 with appropriately corrected leopold i suspect you will find there is indeed stat sig similar trends incl amplification setting up the statistical testing should be interesting with this many combinations regards tom dr thomas r karl lhd director noaas national climatic data center veachbaley federal building 151 patton avenue asheville nc 288015001 tel 828 2714476 fax 828 2714246 thomasrkarlnoaagov mailtothomasrkarlnoaagov benjamin d santer program for climate model diagnosis and intercomparison lawrence livermore national laboratory po box 808 mail stop l103 livermore ca 94550 usa tel 925 4222486 fax 925 4227675 email santer1llnlgov xflowed