from ben santer santer1llnlgov to john lanzante johnlanzantenoaagov thomas r karl thomasrkarlnoaagov carl mears mearsremsscom david c bader bader2llnlgov dian j seidel dianseidelnoaagov francis w zwiers franciszwiersecgcca frank wentz frankwentzremsscom karl taylor taylor13llnlgov leopold haimberger leopoldhaimbergerunivieacat melissa free melissafreenoaagov michael c maccracken mmaccraccomcastnet philip d jones pjonesueaacuk steven sherwood stevensherwoodyaleedu steve klein klein21mailllnlgov susan solomon ssolomonalnoaagov thorne peter peterthornemetofficegovuk tim osborn tosbornueaacuk tom wigley wigleycgducaredu gavin schmidt gschmidtgissnasagov subject more significance testing date thu 27 dec 2007 162619 0800 replyto santer1llnlgov xflowed dear folks this email briefly summarizes the trend significance test results as i mentioned in yesterdays email ive added new case referred to as type3 below ive also added results for tests with stipulated 10 significance level here is the explanation of the four different types of trend test 1 obsvsmodel observed msu trends in rss and uah are tested against trends in synthetic msu data in 49 realizations of the 20c3m experiment results from rss and uah are pooled yielding total of 98 tests for t2 trends and 98 tests for t2lt trends 2 modelvsmodel type1 involves model data only trend in synthetic msu data in each of 49 20c3m realizations is tested against each trend in the remaining 48 realizations ie trend tests involving identical data yields total of 49 x 48 2352 tests the significance of trend differences is function of both intermodel differences in climate sensitivity applied 20c3m forcings and the amplitude of variability and withinmodel effects ie is related to the different manifestations of natural internal variability superimposed on the underlying forced response 3 modelvsmodel type2 involves model data only limited to the m models with multiple realizations of the 20c3m experiment for each of these m models the number of unique combinations c of n 20c3m realizations into r trend pairs is determined for example in the case of n 5 c n rnr 10 the significance of trend differences is solely function of withinmodel effects ie is related to the different manifestations of natural internal variability superimposed on the underlying forced response there are total of 62 tests not 124 as i erroneously reported yesterday 4 modelvsmodel type3 involves model data only for each of the 19 models only the first 20c3m realization is used the trend in each models first 20c3m realization is tested against each trend in the first 20c3m realization of the remaining 18 models yields total of 19 x 18 342 tests the significance of trend differences is solely function of intermodel differences in climate sensitivity applied 20c3m forcings and the amplitude of variability rejection rates for stipulated 5 significance level test type of tests t2 hits t2lt hits 1 obsvsmodel 49 x 2 98 2 204 1 102 2 modelvsmodel type1 49 x 48 2352 58 247 32 136 3 modelvsmodel type2 62 0 000 0 000 4 modelvsmodel type3 19 x 18 342 22 643 14 409 rejection rates for stipulated 10 significance level test type of tests t2 hits t2lt hits 1 obsvsmodel 49 x 2 98 4 408 2 204 2 modelvsmodel type1 49 x 48 2352 80 340 46 196 3 modelvsmodel type2 62 1 161 0 000 4 modelvsmodel type3 19 x 18 342 28 819 20 585 rejection rates for stipulated 20 significance level test type of tests t2 hits t2lt hits 1 obsvsmodel 49 x 2 98 7 714 5 510 2 modelvsmodel type1 49 x 48 2352 176 748 100 425 3 modelvsmodel type2 62 4 645 3 484 4 modelvsmodel type3 19 x 18 342 42 1228 28 819 features of interest as you might expect for each of the three significance levels type3 tests yield the highest rejection rates of the null hypothesis of significant difference in trend type2 tests yield the lowest rejection rates this is simply telling us that the intermodel differences in trends tend to be larger than the betweenrealization differences in trends in any individual model b rejection rates for the modelversusobserved trend tests are consistently lower than for the modelversusmodel type3 tests on average therefore the tropospheric trend differences between the observational datasets used here rss and uah and the synthetic msu temperatures calculated from 19 cmip3 models are actually less significant than the intermodel trend differences arising from differences in sensitivity 20c3m forcings and levels of variability i also thought that it would be fun to use the model data to explore the implications of douglass et als flawed statistical procedure recall that douglass et compare in their table iii the observed t2 and t2lt trends in rss and uah with the overall means of the multimodel distributions of t2 and t2lt trends their standard error sigmase is meant to represent an estimate of the uncertainty of the mean ie the mean trend sigmase is given as sigmase sigma sqrtn 1 where sigma is the standard deviation of the model trends and n is the number of independent models 22 in their case douglass et apparently estimate sigma using ensemblemean trends for each model if 20c3m ensembles are available so what happens if we apply this procedure using model data only this is rather easy to do as above in the type1 type2 and type3 tests i simply used the synthetic msu trends from the 19 cmip3 models employed in our ccsp report and in santer et 2005 so n 19 for each model i calculated the ensemblemean 20c3m trend over 1979 to 1999 where multiple 20c3m realizations were available lets call these mean trends bj where j the index over models 1 2 19 further lets regard b1 as the surrogate observations and then use douglass et als approach to test whether b1 is significantly different from the overall mean of the remaining 18 members of bj then repeat with b2 as surrogate observations etc for each layeraveraged temperature series this yields 19 tests of the significance of differences in mean trends to give you feel for this stuff ive reproduced below the results for tests involving t2lt trends the obs column is the ensemblemean t2lt trend in the surrogate observations modave is the overall mean trend in the 18 remaining members of the distribution and sigma is the 1sigma standard deviation of these trends sigmase is 1 x sigmase note that douglass et give 2 x sigmase in their table iii multiplying our sigmase results by two gives values similar to theirs normd is simply the normalized difference obsmodave sigmase and pvalue is the pvalue for the normalized difference assuming that this difference is approximately normally distributed model obs modave sigma sigmase normd pvalue ccsm30 01580 02179 00910 00215 27918 00052 gfdl20 02576 02124 00915 00216 20977 00359 gfdl21 03567 02069 00854 00201 74404 00000 giss_eh 01477 02185 00906 00214 33153 00009 giss_er 01938 02159 00919 00217 10205 03075 miroc32_t42 01285 02196 00897 00211 43094 00000 miroc32_t106 02298 02139 00920 00217 07305 04651 mri232a 02800 02111 00907 00214 32196 00013 pcm 01496 02184 00907 00214 32170 00013 hadcm3 01936 02159 00919 00217 10327 03018 hadgem1 03099 02095 00891 00210 47784 00000 cccma31 04236 02032 00769 00181 121591 00000 cnrm30 02409 02133 00918 00216 12762 02019 csiro30 02780 02113 00908 00214 31195 00018 echam5 01252 02197 00895 00211 44815 00000 iap_fgoals10 01834 02165 00917 00216 15314 01257 giss_aom 01788 02168 00916 00216 17579 00788 inmcm30 00197 02256 00790 00186 110541 00000 ipsl_cm4 02258 02142 00920 00217 05359 05920 t2lt of pvalues 005 12 rejection rate 6316 t2lt of pvalues 010 13 rejection rate 6842 t2lt of pvalues 020 14 rejection rate 7368 the corresponding rejection rates for the tests involving t2 data are t2 of pvalues 005 12 rejection rate 6316 t2 of pvalues 010 13 rejection rate 6842 t2 of pvalues 020 15 rejection rate 7895 bottom line if we applied douglass et als ridiculous test of difference in mean trends to model data only in fact to virtually the same model data they used in their paper one would conclude that nearly twothirds of the individual models had trends that were significantly different from the multimodel mean trend to follow douglass et als flawed logic this would mean that twothirds of the models really arent models after all happy new year to all of you with best regards ben benjamin d santer program for climate model diagnosis and intercomparison lawrence livermore national laboratory po box 808 mail stop l103 livermore ca 94550 usa tel 925 4222486 fax 925 4227675 email santer1llnlgov xflowed